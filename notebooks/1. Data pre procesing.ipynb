{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def hot2label(hot_array):\n",
    "    '''Convert the one hot encoding into tokens'''\n",
    "    #(hot_array * np.array([0,1,2,3],dtype=np.uint8)).sum(-1,dtype=np.uint8)\n",
    "    return np.einsum('ijk,k->ij',hot_array,np.array([0,1,2,3],dtype=np.uint8)) #two times faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Load data from Matlab file into numpy\n",
    "2. Transpose\n",
    "3. Convert one hot to label\n",
    "4. Save\n",
    "\n",
    "For the train set, we separate the work in chunk to fit data in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#https://www.pythonforthelab.com/blog/how-to-use-hdf5-files-in-python/\n",
    "trainmat = h5py.File('../data/DeepSEA/train.mat') #(1000, 4, 4400000)\n",
    "N = trainmat['trainxdata'].shape[-1] \n",
    "c = trainmat['traindata'].shape[0] \n",
    "seq_len = 1000\n",
    "                     \n",
    "with h5py.File('../data/Processed/train.hdf5', 'w') as train_h5:\n",
    "    X_h5 = train_h5.create_dataset(\"X_train\", (N,seq_len), dtype='u1')#, compression=\"gzip\")\n",
    "    y_h5 = train_h5.create_dataset(\"y_train\", (N,c), dtype='u1')#, compression=\"gzip\")\n",
    "\n",
    "    chunk_sz = 200_000\n",
    "    n_chunks = N//chunk_sz if N%chunk_sz==0 else N//chunk_sz+1\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "        fi = int( i   *chunk_sz)\n",
    "        ti = int((i+1)*chunk_sz) if i!=(n_chunks-1) else N\n",
    "\n",
    "        X_train = np.transpose(trainmat['trainxdata'][:,:,fi:ti],axes=(2,0,1))\n",
    "        y_train = (trainmat['traindata'][:,fi:ti]).T\n",
    "        X_train = hot2label(X_train)\n",
    "#         print(X_train.shape,X_h5.shape,fi,ti,N,seq_len)\n",
    "        #save hdf5\n",
    "        X_h5[fi:ti] = X_train\n",
    "        y_h5[fi:ti] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "validmat = scipy.io.loadmat('../data/DeepSEA/valid.mat')\n",
    "X_valid = np.transpose(validmat['validxdata'],axes=(0,2,1))\n",
    "y_valid = validmat['validdata']\n",
    "X_valid = hot2label(X_valid)\n",
    "np.savez_compressed('../data/Processed/valid',X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "testmat = scipy.io.loadmat('data/DeepSEA/test.mat')\n",
    "X_test = np.transpose(testmat['testxdata'],axes=(0,2,1))\n",
    "y_test = testmat['testdata']\n",
    "X_test  = hot2label(X_test)\n",
    "np.savez_compressed('../data/Processed/test', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#https://www.pythonforthelab.com/blog/how-to-use-hdf5-files-in-python/\n",
    "trainmat = h5py.File('../data/DeepSEA/train.mat') #(1000, 4, 4400000)\n",
    "N = trainmat['trainxdata'].shape[-1] \n",
    "c = trainmat['traindata'].shape[0] \n",
    "seq_len = 1000\n",
    "                     \n",
    "with h5py.File('../data/Processed/hot_train.hdf5', 'w') as train_h5:\n",
    "    X_h5 = train_h5.create_dataset(\"X_train\", (N,seq_len,4), dtype='u1', compression=\"gzip\")\n",
    "    y_h5 = train_h5.create_dataset(\"y_train\", (N,c), dtype='u1', compression=\"gzip\")\n",
    "\n",
    "    chunk_sz = 200_000\n",
    "    n_chunks = N//chunk_sz if N%chunk_sz==0 else N//chunk_sz+1\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "        fi = int( i   *chunk_sz)\n",
    "        ti = int((i+1)*chunk_sz) if i!=(n_chunks-1) else N\n",
    "\n",
    "        X_train = np.transpose(trainmat['trainxdata'][:,:,fi:ti],axes=(2,0,1))\n",
    "        y_train = (trainmat['traindata'][:,fi:ti]).T\n",
    "\n",
    "        X_h5[fi:ti] = X_train\n",
    "        y_h5[fi:ti] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "validmat = scipy.io.loadmat('../data/DeepSEA/valid.mat')\n",
    "X_valid = np.transpose(validmat['validxdata'],axes=(0,2,1))\n",
    "y_valid = validmat['validdata']\n",
    "np.savez_compressed('../data/Processed/hot_valid2',X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testmat = scipy.io.loadmat('../data/DeepSEA/test.mat')\n",
    "X_test = np.transpose(testmat['testxdata'],axes=(0,2,1))\n",
    "y_test = testmat['testdata']\n",
    "np.savez_compressed('../data/Processed/hot_test', X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
