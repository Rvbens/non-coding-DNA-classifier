{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import sys,os,h5py,math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Personal imports\n",
    "sys.path.append(os.path.dirname(os.getcwd())) #add parent folder to PATH\n",
    "import lib.models as models\n",
    "from lib.metrics import accuracy,weighted_aucs\n",
    "\n",
    "chpt_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DanQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        super(CustomDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        \n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i): return self.x[i].long(), self.y[i].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,bs):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.model = models.DanQ()\n",
    "        self.bs = bs #batch size\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        tensorboard_logs = {}#{'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'val_loss': self.loss_fn(y_hat, y)}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {}#{'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_pred = F.softmax(y_hat,dim=1).detach().cpu()\n",
    "        return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred, 'y_true':y.cpu()}\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "        roc_auc = weighted_auc(y_preds,y_trues, self.class_weights)\n",
    "        \n",
    "        tensorboard_logs = {'test_loss': avg_loss,'roc_auc':roc_auc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        trn_ds = CustomDataset(X_train,y_train)\n",
    "        trn_dl = DataLoader(trn_ds, batch_size=self.bs,shuffle=True, num_workers=6)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = CustomDataset(X_valid,y_valid)\n",
    "        vld_dl = DataLoader(vld_ds, batch_size=self.bs,shuffle=False, num_workers=6)\n",
    "        return vld_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test = np.load('../data/Processed/test.npz')\n",
    "        X_test = torch.tensor(test['arr_0'][:])\n",
    "        y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "        tst_ds = CustomDataset(X_test,y_test)\n",
    "        tst_dl = DataLoader(tst_ds, batch_size=self.bs,shuffle=False, num_workers=6)\n",
    "        return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation sanity check:   0%|          | 0/5 [00:00<?, ?batch/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-48c34e243480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.01,val_percent_check=0.5,\n\u001b[1;32m      4\u001b[0m                      default_save_path='../data',log_gpu_memory='min_max')    \n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# #trainer.test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m# ON CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/dp_mixin.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdp_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_progress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_val_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sanity_val_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# close progress bars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop_mixin.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, dataloaders, max_batches, test)\u001b[0m\n\u001b[1;32m     48\u001b[0m                                                  \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                                                  \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                                                  test)\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# track outputs for collation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop_mixin.py\u001b[0m in \u001b[0;36mevaluation_forward\u001b[0;34m(self, model, batch, batch_idx, dataloader_idx, test)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a010879fb08>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   reduction=self.reduction)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2123\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2124\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "exp = Experiment(bs=512)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.01,val_percent_check=0.5,\n",
    "                     default_save_path='../data',log_gpu_memory='min_max')    \n",
    "trainer.fit(exp) \n",
    "# #trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp = Experiment(bs=512)\n",
    "chpt_path = '../data/lightning_logs/version_4/checkpoints/_ckpt_epoch_1.ckpt'\n",
    "exp.load_state_dict(torch.load(chpt_path)['state_dict'])\n",
    "# exp.cuda()\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.1,val_percent_check=0.5,\n",
    "                     test_percent_check=0.1, default_save_path='../data',log_gpu_memory='min_max')\n",
    "trainer.test(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import transformers as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self,x,y,mem_len):\n",
    "        super(LMDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        self.n = x.shape[0]*math.ceil(1000/mem_len)\n",
    "        \n",
    "    def __len__(self): return self.n\n",
    "    def __getitem__(self, i): \n",
    "        (b_idx,seq_idxs, seq_start) = i\n",
    "        x = self.x[b_idx,seq_idxs[0]:seq_idxs[1]].long()\n",
    "        inp,tgt = x[:-1], x[1:]\n",
    "        return inp,tgt,seq_start\n",
    "    \n",
    "class LMSampleR(Sampler):\n",
    "    def __init__(self, ds, bs,mem_len):\n",
    "        self.ds, self.bs = ds, bs\n",
    "        self.mem_len = mem_len\n",
    "        \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(0,self.ds.x.shape[0],self.bs):\n",
    "            for j in range(0,1000,self.mem_len):\n",
    "                seq_idxs = (j,j+self.mem_len+1)\n",
    "                for k in range(self.bs):\n",
    "                    b_idx = i+k\n",
    "                    seq_start = j==0 \n",
    "                    yield (b_idx,seq_idxs, seq_start) # (bs,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TransXL_LM(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super(TransXL_LM, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.core = ts.TransfoXLModel(cfg)\n",
    "        self.lm_head = nn.Linear(self.cfg.d_model,self.cfg.vocab_size)\n",
    "        \n",
    "    def forward(self,x,mems=None):\n",
    "        last_hidden_state,  mems = self.core(x,mems)\n",
    "        out = self.lm_head(last_hidden_state)\n",
    "        return out, mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,bs):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.cfg = ts.TransfoXLConfig(vocab_size=4, d_model=64, d_embed=8, n_head=4, d_head=16, d_inner=128, \n",
    "                             n_layer=6, tgt_len=0, ext_len=0, mem_len=512, cutoffs=[1], )\n",
    "        self.model = TransXL_LM(self.cfg)\n",
    "        self.bs = bs #batch size\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y, new_mem = batch\n",
    "        self.mem = None if new_mem[0] else self.mem\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat.view(-1,self.cfg.vocab_size), y.view(-1))\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, new_mem = batch\n",
    "        self.mem = None if new_mem[0] else self.mem\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        return {'val_loss': self.loss_fn(y_hat.view(-1,self.cfg.vocab_size), y.view(-1))}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         x, y, new_mem = batch\n",
    "#         self.mem = None if new_mem[0] else self.mem\n",
    "#         y_hat, self.mem = self.forward(x)\n",
    "#         y_pred = F.softmax(y_hat,dim=1).detach().cpu()\n",
    "#         return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred, 'y_true':y.cpu()}\n",
    "\n",
    "#     def test_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "#         y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "#         y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "#         roc_auc = weighted_auc(y_preds,y_trues, self.class_weights)\n",
    "        \n",
    "        tensorboard_logs = {'test_loss': avg_loss,'roc_auc':roc_auc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        \n",
    "        trn_ds = LMDataset(X_train,y_train,self.cfg.mem_len)\n",
    "        splr   = LMSampleR(trn_ds,self.bs,self.cfg.mem_len)\n",
    "        trn_dl = DataLoader(trn_ds, self.bs,sampler=splr,pin_memory=True)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = LMDataset(X_valid,y_valid,self.cfg.mem_len)\n",
    "        splr   = LMSampleR(vld_ds,self.bs,self.cfg.mem_len)\n",
    "        vld_dl = DataLoader(vld_ds,self.bs,sampler=splr,pin_memory=True)\n",
    "        return vld_dl\n",
    "    \n",
    "#     @pl.data_loader\n",
    "#     def test_dataloader(self):\n",
    "#         test = np.load('../data/Processed/test.npz')\n",
    "#         X_test = torch.tensor(test['arr_0'][:])\n",
    "#         y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "#         tst_ds = LMDataset(X_test,y_test,self.cfg.mem_len)\n",
    "#         splr   = LMSampleR(tst_ds,self.bs,self.cfg.mem_len)\n",
    "#         tst_dl = DataLoader(tst_ds,self.bs,sampler=splr,pin_memory=True)\n",
    "#         return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp = Experiment(bs=32)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.01,val_percent_check=0.5,\n",
    "                     default_save_path='../data')    \n",
    "\n",
    "trainer.fit(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#add acc\n",
    "#dataloader slow when num_workers>0\n",
    "#add shuffler to samplers\n",
    "#apex https://github.com/adityaiitb/pyprof2\n",
    "#resnet+transformerxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet + TransformerXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        super(CustomDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        \n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i): return self.x[i].long(), self.y[i].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyclic lr with restart\n",
    "def lr_i(step_i,cycle_len = 1000, lrs = (3e-4,1e-5),warm_pct = 4/20):\n",
    "    cycle_i = step_i%cycle_len\n",
    "    warm_len = int(cycle_len*warm_pct)\n",
    "    cool_len = cycle_len - warm_len\n",
    "    lr_range = lrs[0]-lrs[1]\n",
    "    \n",
    "    if cycle_i < warm_len:\n",
    "        return lrs[1] + lr_range*(cycle_i/warm_len)\n",
    "    else:\n",
    "        return lrs[0] - lr_range*((cycle_i-warm_len)/cool_len)\n",
    "    \n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = np.linspace(0,50*30,10000)\n",
    "# y = list(map(lr_i,x))\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.ylim([0,4e-4])\n",
    "# plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/8\n",
    "from matplotlib.lines import Line2D\n",
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            mean = p.grad.abs().mean()\n",
    "            if mean.item() == 0: print(n)\n",
    "            ave_grads.append(mean)\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.0001, top=0.001) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,model,hparams):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.model = model\n",
    "        self.bs = hparams['bs'] #batch size\n",
    "        self.lr = hparams['lr'] #batch size\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "    def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "        # warm up lr\n",
    "        if self.trainer.global_step < 500:\n",
    "            lr_scale = min(1., float(self.trainer.global_step + 1) / 200.)\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr_scale * self.lr\n",
    "\n",
    "#         for pg in optimizer.param_groups:\n",
    "#             pg['lr'] = lr_i(self.trainer.global_step)\n",
    "#         plot_grad_flow(self.model.named_parameters())    \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        y_pred = torch.sigmoid(y_hat)\n",
    "        return {'val_loss': self.loss_fn(y_hat, y),'y_pred':y_pred.cpu(), 'y_true':y.cpu()}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "\n",
    "        roc_auc, pr_auc = weighted_aucs(y_preds.cuda(),y_trues.cuda(), self.class_weights.cuda())\n",
    "        acc = accuracy(y_preds,y_trues)\n",
    "        tensorboard_logs = {'val_loss': avg_loss,'valid_roc_auc':roc_auc,'valid_pr_auc':pr_auc,'valid_acc':acc}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        y_pred = torch.sigmoid(y_hat)\n",
    "        return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred.cpu(), 'y_true':y.cpu()}\n",
    "\n",
    "    def test_end(self, outputs,save_preds=True):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "        if save_preds: self.preds=[y_preds,y_trues]\n",
    "            \n",
    "        roc_auc, pr_auc = weighted_aucs(y_preds.cuda(),y_trues.cuda(), self.class_weights.cuda())\n",
    "        acc = accuracy(y_preds,y_trues)\n",
    "        print('Test ROC AUC:',roc_auc.item(),'Test PR AUC:',pr_auc.item())\n",
    "        tensorboard_logs = {'test_loss': avg_loss,'test_roc_auc':roc_auc,'test_pr_auc':pr_auc,'test_acc':acc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr,betas=(0.5,0.999))\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        \n",
    "        trn_ds = CustomDataset(X_train,y_train)\n",
    "        trn_dl = DataLoader(trn_ds, self.bs,pin_memory=True,shuffle=True)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = CustomDataset(X_valid,y_valid)\n",
    "        vld_dl = DataLoader(vld_ds,self.bs,pin_memory=True)\n",
    "        return vld_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test = np.load('../data/Processed/test.npz')\n",
    "        X_test = torch.tensor(test['arr_0'][:])\n",
    "        y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "        tst_ds = CustomDataset(X_test,y_test)\n",
    "        tst_dl = DataLoader(tst_ds,self.bs,pin_memory=True)\n",
    "        return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\t\t\t\t\n",
      "Resnet part:\t8894k\n",
      "Sequence part:\t3803k\n",
      "Linear part:\t45528k\n",
      "Total:\t\t58226k\n"
     ]
    }
   ],
   "source": [
    "#best t_xl model\n",
    "seq_model = models.TransXL(d_model=352, n_layer=6, n_head=4, d_head=16, d_inner=256)\n",
    "\n",
    "model = models.ResSeqLin(vocab_size=4, d_emb=64, seq_model=seq_model,\n",
    "                   n_res_blocks=3,res_k=16,\n",
    "                   skip_cnt=True, fc_h_dim=512, lin_p=0.5, WVN=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\t\t\t\t\n",
      "Resnet part:\t4710k\n",
      "Sequence part:\t1715k\n",
      "Linear part:\t60052k\n",
      "Total:\t\t66479k\n"
     ]
    }
   ],
   "source": [
    "seq_model = models.BERT(d_model=256, n_layer=4, n_head=4)\n",
    "\n",
    "model = models.ResSeqLin(vocab_size=4, d_emb=64, seq_model=seq_model,\n",
    "                   n_res_blocks=3, res_k=16, res_p=0.1,\n",
    "                   skip_cnt=True, fc_h_dim=925, lin_p=0.05)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\t\t\t\t\n",
      "Resnet part:\t8894k\n",
      "Sequence part:\t1988k\n",
      "Linear part:\t82252k\n",
      "Total:\t\t93135k\n"
     ]
    }
   ],
   "source": [
    "seq_model = models.BiLSTM(d_model=352)\n",
    "\n",
    "model = models.ResSeqLin(vocab_size=4, d_emb=64, seq_model=seq_model,\n",
    "                   n_res_blocks=3, res_k=16, res_p=0.1,\n",
    "                   skip_cnt=True, fc_h_dim=925, lin_p=0.05)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(model,{'bs':25,'lr':1e-4})\n",
    "if chpt_path:\n",
    "    exp.load_state_dict(torch.load(chpt_path)['state_dict'])\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, fast_dev_run=True, max_nb_epochs=10, accumulate_grad_batches=4,\n",
    "                     train_percent_check=1, val_check_interval=0.1, use_amp=model.use_amp,\n",
    "                     default_save_path='../data')    \n",
    "# trainer.fit(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "# op = torch.optim.Adam(model.parameters(), lr=1e-4,betas=(0.5,0.999))\n",
    "# plt.figure(figsize=(20,15))\n",
    "# for i,(x,y) in enumerate(exp.val_dataloader()[0]):\n",
    "#     op.zero_grad()\n",
    "# #     x,y = x.cuda(),  y.cuda()\n",
    "#     y_hat, mem = model(x)\n",
    "#     loss = loss_fn(y_hat, y)\n",
    "#     loss.backward()\n",
    "#     plot_grad_flow(model.named_parameters())\n",
    "#     op.step()\n",
    "#     if i==1:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# find memory leak(loading train dataset on every run)\n",
    "# guardar hparams\n",
    "# torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "# label smoothing\n",
    "# balanced valdiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 18201/18201 [03:42<00:00, 81.95batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.895789623260498 Test PR AUC: 0.43330907821655273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "# https://arxiv.org/pdf/1912.01857.pdf\n",
    "# https://github.com/feidfoe/AdjustBnd4Imbalance/blob/master/cifar.py\n",
    "gamma = 0.1 # hparams for re_scaling https://arxiv.org/pdf/1912.01857.pdf\n",
    "if args.evaluate:\n",
    "    print('\\nEvaluation only')\n",
    "    test_loss, test_acc = test(testloader, model, criterion, \n",
    "                               start_epoch, use_cuda)\n",
    "    print('[w/o RS] Test Loss: %.8f, Test Acc: %.2f%%' % (test_loss, test_acc))\n",
    "\n",
    "    current_state = model.state_dict()\n",
    "    W = current_state['module.fc.weight']\n",
    "\n",
    "    imb_factor = 1. / args.imbalance\n",
    "    img_max = 50000/num_classes\n",
    "    num_sample = [img_max * (imb_factor**(i/(num_classes - 1))) \\\n",
    "                     for i in range(num_classes)]\n",
    "\n",
    "    ns = [ float(n) / max(num_sample) for n in num_sample ]\n",
    "    ns = [ n**gamma for n in ns ]\n",
    "    ns = torch.FloatTensor(ns).unsqueeze(-1).cuda()\n",
    "    new_W = W / ns\n",
    "\n",
    "    current_state['module.fc.weight'] = new_W\n",
    "    model.load_state_dict(current_state)\n",
    "\n",
    "    test_loss, test_acc = test(testloader, model, criterion, \n",
    "                               start_epoch, use_cuda)\n",
    "    print('[w/  RS] Test Loss: %.8f, Test Acc: %.2f%%' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#https://arxiv.org/pdf/1610.02391.pdf\n",
    "# https://github.com/HaebinShin/grad-cam-text\n",
    "# https://course.fast.ai/videos/?lesson=6 1:06:00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlnd",
   "language": "python",
   "name": "mlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
