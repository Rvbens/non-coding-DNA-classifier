{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import sys,os,h5py,math\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Personal imports\n",
    "sys.path.append(os.path.dirname(os.getcwd())) #add parent folder to PATH\n",
    "import lib.models as models\n",
    "from lib.metrics import accuracy,weighted_aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DanQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        super(CustomDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        \n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i): return self.x[i].long(), self.y[i].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,bs):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.model = models.DanQ()\n",
    "        self.bs = bs #batch size\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        tensorboard_logs = {}#{'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'val_loss': self.loss_fn(y_hat, y)}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {}#{'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_pred = F.softmax(y_hat,dim=1).detach().cpu()\n",
    "        return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred, 'y_true':y.cpu()}\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "        roc_auc = weighted_auc(y_preds,y_trues, self.class_weights)\n",
    "        \n",
    "        tensorboard_logs = {'test_loss': avg_loss,'roc_auc':roc_auc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        trn_ds = CustomDataset(X_train,y_train)\n",
    "        trn_dl = DataLoader(trn_ds, batch_size=self.bs,shuffle=True, num_workers=6)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = CustomDataset(X_valid,y_valid)\n",
    "        vld_dl = DataLoader(vld_ds, batch_size=self.bs,shuffle=False, num_workers=6)\n",
    "        return vld_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test = np.load('../data/Processed/test.npz')\n",
    "        X_test = torch.tensor(test['arr_0'][:])\n",
    "        y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "        tst_ds = CustomDataset(X_test,y_test)\n",
    "        tst_dl = DataLoader(tst_ds, batch_size=self.bs,shuffle=False, num_workers=6)\n",
    "        return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  91%|█████████▏| 85/93 [00:20<00:01,  4.26batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 86/93 [00:21<00:02,  2.40batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Epoch 1:  95%|█████████▍| 88/93 [00:21<00:01,  3.22batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Epoch 1:  97%|█████████▋| 90/93 [00:21<00:00,  4.22batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Epoch 1: 100%|██████████| 93/93 [00:22<00:00,  5.38batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "                                                            \u001b[A/home/ruben/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/pt_callbacks.py:250: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  ' skipping.', RuntimeWarning)\n",
      "Epoch 1: : 94batch [00:23,  4.06batch/s, batch_nb=85, gpu=0, loss=0.132, v_nb=22]                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = Experiment(bs=512)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.01,val_percent_check=0.5,\n",
    "                     default_save_path='../data',log_gpu_memory='min_max')    \n",
    "trainer.fit(exp) \n",
    "# #trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 88/88 [00:08<00:00, 10.53batch/s]\n"
     ]
    }
   ],
   "source": [
    "exp = Experiment(bs=512)\n",
    "chpt_path = '../data/lightning_logs/version_4/checkpoints/_ckpt_epoch_1.ckpt'\n",
    "exp.load_state_dict(torch.load(chpt_path)['state_dict'])\n",
    "# exp.cuda()\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.1,val_percent_check=0.5,\n",
    "                     test_percent_check=0.1, default_save_path='../data',log_gpu_memory='min_max')\n",
    "trainer.test(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import transformers as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self,x,y,mem_len):\n",
    "        super(LMDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        self.n = x.shape[0]*math.ceil(1000/mem_len)\n",
    "        \n",
    "    def __len__(self): return self.n\n",
    "    def __getitem__(self, i): \n",
    "        (b_idx,seq_idxs, seq_start) = i\n",
    "        x = self.x[b_idx,seq_idxs[0]:seq_idxs[1]].long()\n",
    "        inp,tgt = x[:-1], x[1:]\n",
    "        return inp,tgt,seq_start\n",
    "    \n",
    "class LMSampleR(Sampler):\n",
    "    def __init__(self, ds, bs,mem_len):\n",
    "        self.ds, self.bs = ds, bs\n",
    "        self.mem_len = mem_len\n",
    "        \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(0,self.ds.x.shape[0],self.bs):\n",
    "            for j in range(0,1000,self.mem_len):\n",
    "                seq_idxs = (j,j+self.mem_len+1)\n",
    "                for k in range(self.bs):\n",
    "                    b_idx = i+k\n",
    "                    seq_start = j==0 \n",
    "                    yield (b_idx,seq_idxs, seq_start) # (bs,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TransXL_LM(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super(TransXL_LM, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.core = ts.TransfoXLModel(cfg)\n",
    "        self.lm_head = nn.Linear(self.cfg.d_model,self.cfg.vocab_size)\n",
    "        \n",
    "    def forward(self,x,mems=None):\n",
    "        last_hidden_state,  mems = self.core(x,mems)\n",
    "        out = self.lm_head(last_hidden_state)\n",
    "        return out, mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,bs):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.cfg = ts.TransfoXLConfig(vocab_size=4, d_model=64, d_embed=8, n_head=4, d_head=16, d_inner=128, \n",
    "                             n_layer=6, tgt_len=0, ext_len=0, mem_len=512, cutoffs=[1], )\n",
    "        self.model = TransXL_LM(self.cfg)\n",
    "        self.bs = bs #batch size\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y, new_mem = batch\n",
    "        self.mem = None if new_mem[0] else self.mem\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat.view(-1,self.cfg.vocab_size), y.view(-1))\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, new_mem = batch\n",
    "        self.mem = None if new_mem[0] else self.mem\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        return {'val_loss': self.loss_fn(y_hat.view(-1,self.cfg.vocab_size), y.view(-1))}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         x, y, new_mem = batch\n",
    "#         self.mem = None if new_mem[0] else self.mem\n",
    "#         y_hat, self.mem = self.forward(x)\n",
    "#         y_pred = F.softmax(y_hat,dim=1).detach().cpu()\n",
    "#         return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred, 'y_true':y.cpu()}\n",
    "\n",
    "#     def test_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "#         y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "#         y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "#         roc_auc = weighted_auc(y_preds,y_trues, self.class_weights)\n",
    "        \n",
    "        tensorboard_logs = {'test_loss': avg_loss,'roc_auc':roc_auc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        \n",
    "        trn_ds = LMDataset(X_train,y_train,self.cfg.mem_len)\n",
    "        splr   = LMSampleR(trn_ds,self.bs,self.cfg.mem_len)\n",
    "        trn_dl = DataLoader(trn_ds, self.bs,sampler=splr,pin_memory=True)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = LMDataset(X_valid,y_valid,self.cfg.mem_len)\n",
    "        splr   = LMSampleR(vld_ds,self.bs,self.cfg.mem_len)\n",
    "        vld_dl = DataLoader(vld_ds,self.bs,sampler=splr,pin_memory=True)\n",
    "        return vld_dl\n",
    "    \n",
    "#     @pl.data_loader\n",
    "#     def test_dataloader(self):\n",
    "#         test = np.load('../data/Processed/test.npz')\n",
    "#         X_test = torch.tensor(test['arr_0'][:])\n",
    "#         y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "#         tst_ds = LMDataset(X_test,y_test,self.cfg.mem_len)\n",
    "#         splr   = LMSampleR(tst_ds,self.bs,self.cfg.mem_len)\n",
    "#         tst_dl = DataLoader(tst_ds,self.bs,sampler=splr,pin_memory=True)\n",
    "#         return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  25%|██▍       | 745/3000 [11:39<34:47,  1.08batch/s, batch_nb=744, gpu=0, loss=1.329, v_nb=14]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-672c6ba16de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      default_save_path='../data')    \n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m# ON CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/dp_mixin.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdp_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# RUN TRAIN STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mbatch_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_step_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;31m# nan grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36moptimizer_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mmodel_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                     \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                     \u001b[0;31m# track metrics for callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/root_module/hooks.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, use_amp, loss, optimizer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp = Experiment(bs=32)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.01,val_percent_check=0.5,\n",
    "                     default_save_path='../data')    \n",
    "\n",
    "trainer.fit(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#add acc\n",
    "#dataloader slow when num_workers>0\n",
    "#add shuffler to samplers\n",
    "#apex https://github.com/adityaiitb/pyprof2\n",
    "#resnet+transformerxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet + TransformerXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        super(CustomDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        \n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i): return self.x[i].long(), self.y[i].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyclic lr with restart\n",
    "def lr_i(step_i,cycle_len = 1000, lrs = (3e-4,1e-5),warm_pct = 4/20):\n",
    "    cycle_i = step_i%cycle_len\n",
    "    warm_len = int(cycle_len*warm_pct)\n",
    "    cool_len = cycle_len - warm_len\n",
    "    lr_range = lrs[0]-lrs[1]\n",
    "    \n",
    "    if cycle_i < warm_len:\n",
    "        return lrs[1] + lr_range*(cycle_i/warm_len)\n",
    "    else:\n",
    "        return lrs[0] - lr_range*((cycle_i-warm_len)/cool_len)\n",
    "    \n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = np.linspace(0,50*30,10000)\n",
    "# y = list(map(lr_i,x))\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.ylim([0,4e-4])\n",
    "# plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,model,hparams):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.model = model\n",
    "        self.bs = hparams['bs'] #batch size\n",
    "        self.lr = hparams['lr'] #batch size\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "    def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "        # warm up lr\n",
    "        if self.trainer.global_step < 500:\n",
    "            lr_scale = min(1., float(self.trainer.global_step + 1) / 200.)\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr_scale * self.lr\n",
    "\n",
    "#         for pg in optimizer.param_groups:\n",
    "#             pg['lr'] = lr_i(self.trainer.global_step)\n",
    "            \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        y_pred = torch.sigmoid(y_hat)\n",
    "        return {'val_loss': self.loss_fn(y_hat, y),'y_pred':y_pred.cpu(), 'y_true':y.cpu()}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "\n",
    "        roc_auc, pr_auc = weighted_aucs(y_preds.cuda(),y_trues.cuda(), self.class_weights.cuda())\n",
    "        acc = accuracy(y_preds,y_trues)\n",
    "        tensorboard_logs = {'val_loss': avg_loss,'valid_roc_auc':roc_auc,'valid_pr_auc':pr_auc,'valid_acc':acc}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        y_pred = torch.sigmoid(y_hat)\n",
    "        return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred.cpu(), 'y_true':y.cpu()}\n",
    "\n",
    "    def test_end(self, outputs,save_preds=True):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "        if save_preds: self.preds=[y_preds,y_trues]\n",
    "            \n",
    "        roc_auc, pr_auc = weighted_aucs(y_preds.cuda(),y_trues.cuda(), self.class_weights.cuda())\n",
    "        acc = accuracy(y_preds,y_trues)\n",
    "        print('Test ROC AUC:',roc_auc.item(),'Test PR AUC:',pr_auc.item())\n",
    "        tensorboard_logs = {'test_loss': avg_loss,'test_roc_auc':roc_auc,'test_pr_auc':pr_auc,'test_acc':acc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        \n",
    "        trn_ds = CustomDataset(X_train,y_train)\n",
    "        trn_dl = DataLoader(trn_ds, self.bs,pin_memory=True,shuffle=True)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = CustomDataset(X_valid,y_valid)\n",
    "        vld_dl = DataLoader(vld_ds,self.bs,pin_memory=True)\n",
    "        return vld_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test = np.load('../data/Processed/test.npz')\n",
    "        X_test = torch.tensor(test['arr_0'][:])\n",
    "        y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "        tst_ds = CustomDataset(X_test,y_test)\n",
    "        tst_dl = DataLoader(tst_ds,self.bs,pin_memory=True)\n",
    "        return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\t\t\t\t\n",
      "Resnet part:\t\t4710k\n",
      "Transformer-XL part:\t1290k\n",
      "Linear part:\t\t33240k\n",
      "Total:\t\t\t39242k\n"
     ]
    }
   ],
   "source": [
    "#best t_xl model\n",
    "cfg = ts.TransfoXLConfig(vocab_size=4, d_embed=8,d_model=256, n_head=4, d_head=16, d_inner=256, \n",
    "                         n_layer=6, tgt_len=0, ext_len=0, mem_len=256, cutoffs=[1], )\n",
    "\n",
    "model = models.ResTransXL(vocab_size=4, d_emb=64, tsfm_cfg=cfg, n_res_blocks=3, res_k=16, \n",
    "                          skip_cnt=True, fc_h_dim=512, lin_p=0.5, WVN=True)\n",
    "chpt_path = '../data/lightning_logs/version_52/checkpoints/_ckpt_epoch_1.ckpt'\n",
    "use_amp = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\t\t\t\t\n",
      "Resnet part:\t\t8894k\n",
      "Transformer-XL part:\t1988k\n",
      "Linear part:\t\t34028k\n",
      "Total:\t\t\t44911k\n"
     ]
    }
   ],
   "source": [
    "cfg = ts.TransfoXLConfig(d_model=352)\n",
    "\n",
    "model = models.ResTransXL(vocab_size=4, d_emb=64, tsfm_cfg=cfg,skip_cnt=True,fc_h_dim=256,n_res_blocks=3, res_k=16\n",
    "                   ,LSTM=True)\n",
    "use_amp = True\n",
    "chpt_path = '../data/lightning_logs/version_55/checkpoints/_ckpt_epoch_5.ckpt' #best LSTM model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp = Experiment(model,{'bs':25,'lr':1e-4})\n",
    "if chpt_path:\n",
    "    exp.load_state_dict(torch.load(chpt_path)['state_dict'])\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, fast_dev_run=False, max_nb_epochs=10, accumulate_grad_batches=4,\n",
    "                     train_percent_check=1, val_check_interval=0.1, use_amp=use_amp,\n",
    "                     default_save_path='../data')    \n",
    "# trainer.fit(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# find memory leak(loading train dataset on every run)\n",
    "# torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "# label smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 18201/18201 [03:03<00:00, 98.93batch/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.889613687992096 Test PR AUC: 0.41424208879470825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "# https://arxiv.org/pdf/1912.01857.pdf\n",
    "# https://github.com/feidfoe/AdjustBnd4Imbalance/blob/master/cifar.py\n",
    "gamma = 0.1 # hparams for re_scaling https://arxiv.org/pdf/1912.01857.pdf\n",
    "if args.evaluate:\n",
    "    print('\\nEvaluation only')\n",
    "    test_loss, test_acc = test(testloader, model, criterion, \n",
    "                               start_epoch, use_cuda)\n",
    "    print('[w/o RS] Test Loss: %.8f, Test Acc: %.2f%%' % (test_loss, test_acc))\n",
    "\n",
    "    current_state = model.state_dict()\n",
    "    W = current_state['module.fc.weight']\n",
    "\n",
    "    imb_factor = 1. / args.imbalance\n",
    "    img_max = 50000/num_classes\n",
    "    num_sample = [img_max * (imb_factor**(i/(num_classes - 1))) \\\n",
    "                     for i in range(num_classes)]\n",
    "\n",
    "    ns = [ float(n) / max(num_sample) for n in num_sample ]\n",
    "    ns = [ n**gamma for n in ns ]\n",
    "    ns = torch.FloatTensor(ns).unsqueeze(-1).cuda()\n",
    "    new_W = W / ns\n",
    "\n",
    "    current_state['module.fc.weight'] = new_W\n",
    "    model.load_state_dict(current_state)\n",
    "\n",
    "    test_loss, test_acc = test(testloader, model, criterion, \n",
    "                               start_epoch, use_cuda)\n",
    "    print('[w/  RS] Test Loss: %.8f, Test Acc: %.2f%%' % (test_loss, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlnd",
   "language": "python",
   "name": "mlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
