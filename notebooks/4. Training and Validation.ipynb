{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import sys,os,h5py,math\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Personal imports\n",
    "sys.path.append(os.path.dirname(os.getcwd())) #add parent folder to PATH\n",
    "import lib.models as models\n",
    "from lib.metrics import accuracy,weighted_aucs\n",
    "\n",
    "chpt_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DanQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        super(CustomDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        \n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i): return self.x[i].long(), self.y[i].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,bs):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.model = models.DanQ()\n",
    "        self.bs = bs #batch size\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        tensorboard_logs = {}#{'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'val_loss': self.loss_fn(y_hat, y)}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {}#{'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_pred = F.softmax(y_hat,dim=1).detach().cpu()\n",
    "        return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred, 'y_true':y.cpu()}\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "        roc_auc = weighted_auc(y_preds,y_trues, self.class_weights)\n",
    "        \n",
    "        tensorboard_logs = {'test_loss': avg_loss,'roc_auc':roc_auc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        trn_ds = CustomDataset(X_train,y_train)\n",
    "        trn_dl = DataLoader(trn_ds, batch_size=self.bs,shuffle=True, num_workers=6)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = CustomDataset(X_valid,y_valid)\n",
    "        vld_dl = DataLoader(vld_ds, batch_size=self.bs,shuffle=False, num_workers=6)\n",
    "        return vld_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test = np.load('../data/Processed/test.npz')\n",
    "        X_test = torch.tensor(test['arr_0'][:])\n",
    "        y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "        tst_ds = CustomDataset(X_test,y_test)\n",
    "        tst_dl = DataLoader(tst_ds, batch_size=self.bs,shuffle=False, num_workers=6)\n",
    "        return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  91%|█████████▏| 85/93 [00:20<00:01,  4.26batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 86/93 [00:21<00:02,  2.40batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Epoch 1:  95%|█████████▍| 88/93 [00:21<00:01,  3.22batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Epoch 1:  97%|█████████▋| 90/93 [00:21<00:00,  4.22batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Epoch 1: 100%|██████████| 93/93 [00:22<00:00,  5.38batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "                                                            \u001b[A/home/ruben/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/pt_callbacks.py:250: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  ' skipping.', RuntimeWarning)\n",
      "Epoch 1: : 94batch [00:23,  4.06batch/s, batch_nb=85, gpu=0, loss=0.132, v_nb=22]                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = Experiment(bs=512)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.01,val_percent_check=0.5,\n",
    "                     default_save_path='../data',log_gpu_memory='min_max')    \n",
    "trainer.fit(exp) \n",
    "# #trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 88/88 [00:08<00:00, 10.53batch/s]\n"
     ]
    }
   ],
   "source": [
    "exp = Experiment(bs=512)\n",
    "chpt_path = '../data/lightning_logs/version_4/checkpoints/_ckpt_epoch_1.ckpt'\n",
    "exp.load_state_dict(torch.load(chpt_path)['state_dict'])\n",
    "# exp.cuda()\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.1,val_percent_check=0.5,\n",
    "                     test_percent_check=0.1, default_save_path='../data',log_gpu_memory='min_max')\n",
    "trainer.test(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import transformers as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self,x,y,mem_len):\n",
    "        super(LMDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        self.n = x.shape[0]*math.ceil(1000/mem_len)\n",
    "        \n",
    "    def __len__(self): return self.n\n",
    "    def __getitem__(self, i): \n",
    "        (b_idx,seq_idxs, seq_start) = i\n",
    "        x = self.x[b_idx,seq_idxs[0]:seq_idxs[1]].long()\n",
    "        inp,tgt = x[:-1], x[1:]\n",
    "        return inp,tgt,seq_start\n",
    "    \n",
    "class LMSampleR(Sampler):\n",
    "    def __init__(self, ds, bs,mem_len):\n",
    "        self.ds, self.bs = ds, bs\n",
    "        self.mem_len = mem_len\n",
    "        \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(0,self.ds.x.shape[0],self.bs):\n",
    "            for j in range(0,1000,self.mem_len):\n",
    "                seq_idxs = (j,j+self.mem_len+1)\n",
    "                for k in range(self.bs):\n",
    "                    b_idx = i+k\n",
    "                    seq_start = j==0 \n",
    "                    yield (b_idx,seq_idxs, seq_start) # (bs,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TransXL_LM(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super(TransXL_LM, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.core = ts.TransfoXLModel(cfg)\n",
    "        self.lm_head = nn.Linear(self.cfg.d_model,self.cfg.vocab_size)\n",
    "        \n",
    "    def forward(self,x,mems=None):\n",
    "        last_hidden_state,  mems = self.core(x,mems)\n",
    "        out = self.lm_head(last_hidden_state)\n",
    "        return out, mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,bs):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.cfg = ts.TransfoXLConfig(vocab_size=4, d_model=64, d_embed=8, n_head=4, d_head=16, d_inner=128, \n",
    "                             n_layer=6, tgt_len=0, ext_len=0, mem_len=512, cutoffs=[1], )\n",
    "        self.model = TransXL_LM(self.cfg)\n",
    "        self.bs = bs #batch size\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y, new_mem = batch\n",
    "        self.mem = None if new_mem[0] else self.mem\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat.view(-1,self.cfg.vocab_size), y.view(-1))\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, new_mem = batch\n",
    "        self.mem = None if new_mem[0] else self.mem\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        return {'val_loss': self.loss_fn(y_hat.view(-1,self.cfg.vocab_size), y.view(-1))}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         x, y, new_mem = batch\n",
    "#         self.mem = None if new_mem[0] else self.mem\n",
    "#         y_hat, self.mem = self.forward(x)\n",
    "#         y_pred = F.softmax(y_hat,dim=1).detach().cpu()\n",
    "#         return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred, 'y_true':y.cpu()}\n",
    "\n",
    "#     def test_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "#         y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "#         y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "#         roc_auc = weighted_auc(y_preds,y_trues, self.class_weights)\n",
    "        \n",
    "        tensorboard_logs = {'test_loss': avg_loss,'roc_auc':roc_auc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        \n",
    "        trn_ds = LMDataset(X_train,y_train,self.cfg.mem_len)\n",
    "        splr   = LMSampleR(trn_ds,self.bs,self.cfg.mem_len)\n",
    "        trn_dl = DataLoader(trn_ds, self.bs,sampler=splr,pin_memory=True)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = LMDataset(X_valid,y_valid,self.cfg.mem_len)\n",
    "        splr   = LMSampleR(vld_ds,self.bs,self.cfg.mem_len)\n",
    "        vld_dl = DataLoader(vld_ds,self.bs,sampler=splr,pin_memory=True)\n",
    "        return vld_dl\n",
    "    \n",
    "#     @pl.data_loader\n",
    "#     def test_dataloader(self):\n",
    "#         test = np.load('../data/Processed/test.npz')\n",
    "#         X_test = torch.tensor(test['arr_0'][:])\n",
    "#         y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "#         tst_ds = LMDataset(X_test,y_test,self.cfg.mem_len)\n",
    "#         splr   = LMSampleR(tst_ds,self.bs,self.cfg.mem_len)\n",
    "#         tst_dl = DataLoader(tst_ds,self.bs,sampler=splr,pin_memory=True)\n",
    "#         return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  25%|██▍       | 745/3000 [11:39<34:47,  1.08batch/s, batch_nb=744, gpu=0, loss=1.329, v_nb=14]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-672c6ba16de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      default_save_path='../data')    \n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m# ON CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/dp_mixin.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdp_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# RUN TRAIN STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mbatch_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_step_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;31m# nan grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36moptimizer_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mmodel_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                     \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                     \u001b[0;31m# track metrics for callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/root_module/hooks.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, use_amp, loss, optimizer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp = Experiment(bs=32)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.01,val_percent_check=0.5,\n",
    "                     default_save_path='../data')    \n",
    "\n",
    "trainer.fit(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#add acc\n",
    "#dataloader slow when num_workers>0\n",
    "#add shuffler to samplers\n",
    "#apex https://github.com/adityaiitb/pyprof2\n",
    "#resnet+transformerxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet + TransformerXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        super(CustomDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        \n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i): return self.x[i].long(), self.y[i].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyclic lr with restart\n",
    "def lr_i(step_i,cycle_len = 1000, lrs = (3e-4,1e-5),warm_pct = 4/20):\n",
    "    cycle_i = step_i%cycle_len\n",
    "    warm_len = int(cycle_len*warm_pct)\n",
    "    cool_len = cycle_len - warm_len\n",
    "    lr_range = lrs[0]-lrs[1]\n",
    "    \n",
    "    if cycle_i < warm_len:\n",
    "        return lrs[1] + lr_range*(cycle_i/warm_len)\n",
    "    else:\n",
    "        return lrs[0] - lr_range*((cycle_i-warm_len)/cool_len)\n",
    "    \n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = np.linspace(0,50*30,10000)\n",
    "# y = list(map(lr_i,x))\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.ylim([0,4e-4])\n",
    "# plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,model,hparams):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.model = model\n",
    "        self.bs = hparams['bs'] #batch size\n",
    "        self.lr = hparams['lr'] #batch size\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "    def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "        # warm up lr\n",
    "        if self.trainer.global_step < 500:\n",
    "            lr_scale = min(1., float(self.trainer.global_step + 1) / 200.)\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr_scale * self.lr\n",
    "\n",
    "#         for pg in optimizer.param_groups:\n",
    "#             pg['lr'] = lr_i(self.trainer.global_step)\n",
    "            \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        y_pred = torch.sigmoid(y_hat)\n",
    "        return {'val_loss': self.loss_fn(y_hat, y),'y_pred':y_pred.cpu(), 'y_true':y.cpu()}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "\n",
    "        roc_auc, pr_auc = weighted_aucs(y_preds.cuda(),y_trues.cuda(), self.class_weights.cuda())\n",
    "        acc = accuracy(y_preds,y_trues)\n",
    "        tensorboard_logs = {'val_loss': avg_loss,'valid_roc_auc':roc_auc,'valid_pr_auc':pr_auc,'valid_acc':acc}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        y_pred = torch.sigmoid(y_hat)\n",
    "        return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred.cpu(), 'y_true':y.cpu()}\n",
    "\n",
    "    def test_end(self, outputs,save_preds=True):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "        if save_preds: self.preds=[y_preds,y_trues]\n",
    "            \n",
    "        roc_auc, pr_auc = weighted_aucs(y_preds.cuda(),y_trues.cuda(), self.class_weights.cuda())\n",
    "        acc = accuracy(y_preds,y_trues)\n",
    "        print('Test ROC AUC:',roc_auc.item(),'Test PR AUC:',pr_auc.item())\n",
    "        tensorboard_logs = {'test_loss': avg_loss,'test_roc_auc':roc_auc,'test_pr_auc':pr_auc,'test_acc':acc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=self.lr)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        \n",
    "        trn_ds = CustomDataset(X_train,y_train)\n",
    "        trn_dl = DataLoader(trn_ds, self.bs,pin_memory=True,shuffle=True)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = CustomDataset(X_valid,y_valid)\n",
    "        vld_dl = DataLoader(vld_ds,self.bs,pin_memory=True)\n",
    "        return vld_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test = np.load('../data/Processed/test.npz')\n",
    "        X_test = torch.tensor(test['arr_0'][:])\n",
    "        y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "        tst_ds = CustomDataset(X_test,y_test)\n",
    "        tst_dl = DataLoader(tst_ds,self.bs,pin_memory=True)\n",
    "        return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best t_xl model\n",
    "# cfg = ts.TransfoXLConfig(vocab_size=4, d_embed=8,d_model=256, n_head=4, d_head=16, d_inner=256, \n",
    "#                          n_layer=6, tgt_len=0, ext_len=0, mem_len=256, cutoffs=[1], )\n",
    "\n",
    "# model = models.ResTransXL(vocab_size=4, d_emb=64, tsfm_cfg=cfg, n_res_blocks=3, res_k=16, \n",
    "#                           skip_cnt=True, fc_h_dim=512, lin_p=0.5, WVN=True)\n",
    "# chpt_path = '../data/lightning_logs/version_52/checkpoints/_ckpt_epoch_1.ckpt'\n",
    "# use_amp = False\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\t\t\t\t\n",
      "Resnet part:\t\t8894k\n",
      "Transformer-XL part:\t1988k\n",
      "Linear part:\t\t122952k\n",
      "Total:\t\t\t133835k\n"
     ]
    }
   ],
   "source": [
    "cfg = ts.TransfoXLConfig(d_model=352)\n",
    "\n",
    "model = models.ResTransXL(vocab_size=4, d_emb=64, tsfm_cfg=cfg,skip_cnt=True,fc_h_dim=925,\n",
    "                          n_res_blocks=3, res_k=16,LSTM=True, LSTM_p=0.25,res_p=0.1,lin_p=0.05)\n",
    "use_amp = True\n",
    "chpt_path = '../data/lightning_logs/version_67/checkpoints/_ckpt_epoch_1.ckpt' #best LSTM model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   9%|▉         | 16007/179200 [18:12<2:57:43, 15.30batch/s, batch_nb=16006, gpu=0, loss=0.056, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|▉         | 17600/179200 [20:02<2:59:12, 15.03batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17602/179200 [20:02<2:36:03, 17.26batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17610/179200 [20:02<1:59:49, 22.48batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17618/179200 [20:03<1:34:33, 28.48batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17626/179200 [20:03<1:16:43, 35.10batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17635/179200 [20:03<1:03:18, 42.53batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17644/179200 [20:03<54:19, 49.57batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]  \n",
      "Epoch 1:  10%|▉         | 17652/179200 [20:03<48:57, 54.99batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17660/179200 [20:03<44:43, 60.20batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17668/179200 [20:03<41:35, 64.72batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17676/179200 [20:03<39:33, 68.05batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17684/179200 [20:03<38:14, 70.38batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17692/179200 [20:03<37:25, 71.92batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17700/179200 [20:04<36:35, 73.55batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17708/179200 [20:04<36:08, 74.48batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17716/179200 [20:04<35:45, 75.28batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17724/179200 [20:04<36:21, 74.02batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17732/179200 [20:04<36:18, 74.12batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17740/179200 [20:04<36:11, 74.35batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17748/179200 [20:04<36:12, 74.31batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17756/179200 [20:04<36:10, 74.38batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17764/179200 [20:04<35:44, 75.27batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17773/179200 [20:05<35:02, 76.78batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17782/179200 [20:05<34:19, 78.38batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17790/179200 [20:05<34:47, 77.34batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17798/179200 [20:05<35:04, 76.71batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17806/179200 [20:05<35:08, 76.53batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17814/179200 [20:05<35:35, 75.56batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17822/179200 [20:05<35:12, 76.39batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17830/179200 [20:05<34:47, 77.30batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17838/179200 [20:05<34:59, 76.87batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17846/179200 [20:05<34:55, 77.00batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17854/179200 [20:06<34:53, 77.06batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17863/179200 [20:06<34:20, 78.31batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17871/179200 [20:06<34:34, 77.78batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17879/179200 [20:06<34:42, 77.46batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17887/179200 [20:06<34:23, 78.16batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17895/179200 [20:06<34:39, 77.57batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17903/179200 [20:06<34:30, 77.90batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|▉         | 17911/179200 [20:06<34:24, 78.14batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  10%|█         | 17920/179200 [20:08<33:57, 79.16batch/s, batch_nb=17599, gpu=0, loss=0.055, v_nb=68]\n",
      "                                                                \u001b[A/home/ruben/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer_io.py:210: UserWarning: Did not find hyperparameters at model.hparams. Saving checkpoint without hyperparameters\n",
      "  \"Did not find hyperparameters at model.hparams. Saving checkpoint without\"\n",
      "Epoch 1:  11%|█         | 18842/179200 [21:17<2:57:27, 15.06batch/s, batch_nb=18521, gpu=0, loss=0.055, v_nb=68] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  12%|█▏        | 21527/179200 [24:25<2:55:00, 15.02batch/s, batch_nb=21206, gpu=0, loss=0.054, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|█▎        | 23911/179200 [27:13<2:53:20, 14.93batch/s, batch_nb=23590, gpu=0, loss=0.054, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  14%|█▍        | 25927/179200 [29:35<2:53:53, 14.69batch/s, batch_nb=25606, gpu=0, loss=0.054, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  16%|█▌        | 28859/179200 [33:01<2:47:09, 14.99batch/s, batch_nb=28538, gpu=0, loss=0.055, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  18%|█▊        | 32055/179200 [36:46<2:49:52, 14.44batch/s, batch_nb=31734, gpu=0, loss=0.054, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▉        | 34519/179200 [39:41<2:40:34, 15.02batch/s, batch_nb=34198, gpu=0, loss=0.054, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  20%|█▉        | 35520/179200 [40:53<2:44:53, 14.52batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35521/179200 [40:53<2:33:18, 15.62batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35530/179200 [40:53<1:56:01, 20.64batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35538/179200 [40:53<1:30:19, 26.51batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35547/179200 [40:54<1:11:53, 33.30batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35556/179200 [40:54<59:07, 40.49batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]  \n",
      "Epoch 1:  20%|█▉        | 35565/179200 [40:54<50:22, 47.52batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35573/179200 [40:54<44:23, 53.93batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35581/179200 [40:54<40:27, 59.17batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35589/179200 [40:54<37:49, 63.28batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35597/179200 [40:54<35:54, 66.64batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35605/179200 [40:54<34:17, 69.80batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35613/179200 [40:54<33:15, 71.96batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35621/179200 [40:55<32:24, 73.83batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35630/179200 [40:55<31:34, 75.78batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35639/179200 [40:55<30:43, 77.86batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35647/179200 [40:55<30:51, 77.54batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35656/179200 [40:55<30:28, 78.49batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35665/179200 [40:55<30:13, 79.16batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35674/179200 [40:55<29:40, 80.62batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35683/179200 [40:55<29:31, 81.03batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35692/179200 [40:55<29:25, 81.27batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35701/179200 [40:56<29:36, 80.77batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35710/179200 [40:56<29:18, 81.59batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35719/179200 [40:56<29:08, 82.07batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35728/179200 [40:56<29:37, 80.72batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35737/179200 [40:56<29:50, 80.12batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35746/179200 [40:56<30:13, 79.10batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35755/179200 [40:56<30:01, 79.63batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35763/179200 [40:56<30:01, 79.64batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35771/179200 [40:56<30:21, 78.74batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35779/179200 [40:56<30:38, 78.01batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35787/179200 [40:57<30:48, 77.59batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35795/179200 [40:57<30:50, 77.50batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35803/179200 [40:57<30:48, 77.58batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35811/179200 [40:57<31:03, 76.96batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35820/179200 [40:57<30:39, 77.95batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35828/179200 [40:57<30:48, 77.57batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|█▉        | 35836/179200 [40:57<30:33, 78.19batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  20%|██        | 35840/179200 [40:59<30:33, 78.19batch/s, batch_nb=35199, gpu=0, loss=0.055, v_nb=68]\n",
      "Epoch 1:  21%|██        | 37539/179200 [43:07<2:44:15, 14.37batch/s, batch_nb=36898, gpu=0, loss=0.055, v_nb=68] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  22%|██▏       | 39735/179200 [45:44<2:42:43, 14.28batch/s, batch_nb=39094, gpu=0, loss=0.055, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  24%|██▍       | 43242/179200 [49:56<2:57:11, 12.79batch/s, batch_nb=42600, gpu=0, loss=0.055, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  26%|██▌       | 45859/179200 [53:05<2:33:29, 14.48batch/s, batch_nb=45218, gpu=0, loss=0.054, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  27%|██▋       | 48055/179200 [55:44<2:35:04, 14.10batch/s, batch_nb=47414, gpu=0, loss=0.055, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  28%|██▊       | 50511/179200 [58:42<2:22:12, 15.08batch/s, batch_nb=49870, gpu=0, loss=0.056, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  30%|██▉       | 53440/179200 [1:02:16<2:24:14, 14.53batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 1:  30%|██▉       | 53447/179200 [1:02:16<1:50:50, 18.91batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53455/179200 [1:02:16<1:25:34, 24.49batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53463/179200 [1:02:16<1:07:56, 30.84batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53472/179200 [1:02:16<55:21, 37.85batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]  \n",
      "Epoch 1:  30%|██▉       | 53481/179200 [1:02:16<46:19, 45.24batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53490/179200 [1:02:16<40:07, 52.22batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53499/179200 [1:02:16<35:42, 58.66batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53507/179200 [1:02:17<33:34, 62.41batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53515/179200 [1:02:17<31:49, 65.84batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53523/179200 [1:02:17<31:13, 67.09batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53531/179200 [1:02:17<30:00, 69.81batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53540/179200 [1:02:17<28:49, 72.68batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53548/179200 [1:02:17<28:18, 73.99batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53557/179200 [1:02:17<27:22, 76.49batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53565/179200 [1:02:17<27:04, 77.33batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53574/179200 [1:02:17<26:42, 78.38batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53583/179200 [1:02:18<26:20, 79.48batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53592/179200 [1:02:18<25:56, 80.70batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53601/179200 [1:02:18<25:47, 81.15batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53610/179200 [1:02:18<25:46, 81.21batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53619/179200 [1:02:18<25:45, 81.23batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53628/179200 [1:02:18<26:07, 80.13batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53637/179200 [1:02:18<26:21, 79.37batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53645/179200 [1:02:18<26:34, 78.75batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53653/179200 [1:02:18<26:39, 78.48batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53661/179200 [1:02:19<26:59, 77.50batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53669/179200 [1:02:19<27:08, 77.10batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53678/179200 [1:02:19<26:39, 78.46batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53687/179200 [1:02:19<26:16, 79.61batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53696/179200 [1:02:19<26:08, 80.02batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53705/179200 [1:02:19<25:59, 80.49batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53714/179200 [1:02:19<25:56, 80.62batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53723/179200 [1:02:19<26:10, 79.87batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53731/179200 [1:02:19<26:21, 79.32batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53739/179200 [1:02:20<26:22, 79.28batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|██▉       | 53748/179200 [1:02:20<26:14, 79.70batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|███       | 53760/179200 [1:02:22<26:14, 79.68batch/s, batch_nb=52799, gpu=0, loss=0.054, v_nb=68]\n",
      "Epoch 1:  30%|███       | 54247/179200 [1:02:57<2:26:21, 14.23batch/s, batch_nb=53286, gpu=0, loss=0.055, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  32%|███▏      | 56455/179200 [1:05:36<2:16:06, 15.03batch/s, batch_nb=55494, gpu=0, loss=0.056, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 59047/179200 [1:08:37<2:13:35, 14.99batch/s, batch_nb=58086, gpu=0, loss=0.054, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  34%|███▍      | 60555/179200 [1:10:22<2:17:53, 14.34batch/s, batch_nb=59594, gpu=0, loss=0.054, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  36%|███▌      | 64807/179200 [1:15:18<2:03:55, 15.38batch/s, batch_nb=63846, gpu=0, loss=0.055, v_nb=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  38%|███▊      | 68838/179200 [1:19:57<2:02:49, 14.97batch/s, batch_nb=67877, gpu=0, loss=0.056, v_nb=68]"
     ]
    }
   ],
   "source": [
    "exp = Experiment(model,{'bs':25,'lr':1e-4})\n",
    "if chpt_path:\n",
    "    exp.load_state_dict(torch.load(chpt_path)['state_dict'])\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, fast_dev_run=False, max_nb_epochs=10, accumulate_grad_batches=4,\n",
    "                     train_percent_check=1, val_check_interval=0.1, use_amp=use_amp,\n",
    "                     default_save_path='../data')    \n",
    "trainer.fit(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# find memory leak(loading train dataset on every run)\n",
    "# guardar hparams\n",
    "# torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "# label smoothing\n",
    "# balanced valdiation\n",
    "# grad cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 18201/18201 [03:03<00:00, 98.93batch/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.889613687992096 Test PR AUC: 0.41424208879470825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "# https://arxiv.org/pdf/1912.01857.pdf\n",
    "# https://github.com/feidfoe/AdjustBnd4Imbalance/blob/master/cifar.py\n",
    "gamma = 0.1 # hparams for re_scaling https://arxiv.org/pdf/1912.01857.pdf\n",
    "if args.evaluate:\n",
    "    print('\\nEvaluation only')\n",
    "    test_loss, test_acc = test(testloader, model, criterion, \n",
    "                               start_epoch, use_cuda)\n",
    "    print('[w/o RS] Test Loss: %.8f, Test Acc: %.2f%%' % (test_loss, test_acc))\n",
    "\n",
    "    current_state = model.state_dict()\n",
    "    W = current_state['module.fc.weight']\n",
    "\n",
    "    imb_factor = 1. / args.imbalance\n",
    "    img_max = 50000/num_classes\n",
    "    num_sample = [img_max * (imb_factor**(i/(num_classes - 1))) \\\n",
    "                     for i in range(num_classes)]\n",
    "\n",
    "    ns = [ float(n) / max(num_sample) for n in num_sample ]\n",
    "    ns = [ n**gamma for n in ns ]\n",
    "    ns = torch.FloatTensor(ns).unsqueeze(-1).cuda()\n",
    "    new_W = W / ns\n",
    "\n",
    "    current_state['module.fc.weight'] = new_W\n",
    "    model.load_state_dict(current_state)\n",
    "\n",
    "    test_loss, test_acc = test(testloader, model, criterion, \n",
    "                               start_epoch, use_cuda)\n",
    "    print('[w/  RS] Test Loss: %.8f, Test Acc: %.2f%%' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#https://arxiv.org/pdf/1610.02391.pdf\n",
    "# https://github.com/HaebinShin/grad-cam-text\n",
    "# https://course.fast.ai/videos/?lesson=6 1:06:00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlnd",
   "language": "python",
   "name": "mlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
