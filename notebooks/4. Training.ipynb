{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import sys,os,h5py,math\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Personal imports\n",
    "sys.path.append(os.path.dirname(os.getcwd())) #add parent folder to PATH\n",
    "import lib.models as models\n",
    "from lib.metrics import accuracy,weighted_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DanQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        super(CustomDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        \n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i): return self.x[i].long(), self.y[i].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,bs):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.model = models.DanQ()\n",
    "        self.bs = bs #batch size\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        tensorboard_logs = {}#{'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'val_loss': self.loss_fn(y_hat, y)}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {}#{'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_pred = F.softmax(y_hat,dim=1).detach().cpu()\n",
    "        return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred, 'y_true':y.cpu()}\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "        roc_auc = weighted_auc(y_preds,y_trues, self.class_weights)\n",
    "        \n",
    "        tensorboard_logs = {'test_loss': avg_loss,'roc_auc':roc_auc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        trn_ds = CustomDataset(X_train,y_train)\n",
    "        trn_dl = DataLoader(trn_ds, batch_size=self.bs,shuffle=True, num_workers=6)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = CustomDataset(X_valid,y_valid)\n",
    "        vld_dl = DataLoader(vld_ds, batch_size=self.bs,shuffle=False, num_workers=6)\n",
    "        return vld_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test = np.load('../data/Processed/test.npz')\n",
    "        X_test = torch.tensor(test['arr_0'][:])\n",
    "        y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "        tst_ds = CustomDataset(X_test,y_test)\n",
    "        tst_dl = DataLoader(tst_ds, batch_size=self.bs,shuffle=False, num_workers=6)\n",
    "        return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  91%|█████████▏| 85/93 [00:20<00:01,  4.26batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 86/93 [00:21<00:02,  2.40batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Epoch 1:  95%|█████████▍| 88/93 [00:21<00:01,  3.22batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Epoch 1:  97%|█████████▋| 90/93 [00:21<00:00,  4.22batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "Epoch 1: 100%|██████████| 93/93 [00:22<00:00,  5.38batch/s, batch_nb=84, gpu=0, loss=0.133, v_nb=22]\n",
      "                                                            \u001b[A/home/ruben/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/pt_callbacks.py:250: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  ' skipping.', RuntimeWarning)\n",
      "Epoch 1: : 94batch [00:23,  4.06batch/s, batch_nb=85, gpu=0, loss=0.132, v_nb=22]                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = Experiment(bs=512)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.01,val_percent_check=0.5,\n",
    "                     default_save_path='../data',log_gpu_memory='min_max')    \n",
    "trainer.fit(exp) \n",
    "# #trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 88/88 [00:08<00:00, 10.53batch/s]\n"
     ]
    }
   ],
   "source": [
    "exp = Experiment(bs=512)\n",
    "chpt_path = '../data/lightning_logs/version_4/checkpoints/_ckpt_epoch_1.ckpt'\n",
    "exp.load_state_dict(torch.load(chpt_path)['state_dict'])\n",
    "# exp.cuda()\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.1,val_percent_check=0.5,\n",
    "                     test_percent_check=0.1, default_save_path='../data',log_gpu_memory='min_max')\n",
    "trainer.test(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import transformers as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self,x,y,mem_len):\n",
    "        super(LMDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        self.n = x.shape[0]*math.ceil(1000/mem_len)\n",
    "        \n",
    "    def __len__(self): return self.n\n",
    "    def __getitem__(self, i): \n",
    "        (b_idx,seq_idxs, seq_start) = i\n",
    "        x = self.x[b_idx,seq_idxs[0]:seq_idxs[1]].long()\n",
    "        inp,tgt = x[:-1], x[1:]\n",
    "        return inp,tgt,seq_start\n",
    "    \n",
    "class LMSampleR(Sampler):\n",
    "    def __init__(self, ds, bs,mem_len):\n",
    "        self.ds, self.bs = ds, bs\n",
    "        self.mem_len = mem_len\n",
    "        \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(0,self.ds.x.shape[0],self.bs):\n",
    "            for j in range(0,1000,self.mem_len):\n",
    "                seq_idxs = (j,j+self.mem_len+1)\n",
    "                for k in range(self.bs):\n",
    "                    b_idx = i+k\n",
    "                    seq_start = j==0 \n",
    "                    yield (b_idx,seq_idxs, seq_start) # (bs,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TransXL_LM(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super(TransXL_LM, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.core = ts.TransfoXLModel(cfg)\n",
    "        self.lm_head = nn.Linear(self.cfg.d_model,self.cfg.vocab_size)\n",
    "        \n",
    "    def forward(self,x,mems=None):\n",
    "        last_hidden_state,  mems = self.core(x,mems)\n",
    "        out = self.lm_head(last_hidden_state)\n",
    "        return out, mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,bs):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.cfg = ts.TransfoXLConfig(vocab_size=4, d_model=64, d_embed=8, n_head=4, d_head=16, d_inner=128, \n",
    "                             n_layer=6, tgt_len=0, ext_len=0, mem_len=512, cutoffs=[1], )\n",
    "        self.model = TransXL_LM(self.cfg)\n",
    "        self.bs = bs #batch size\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y, new_mem = batch\n",
    "        self.mem = None if new_mem[0] else self.mem\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat.view(-1,self.cfg.vocab_size), y.view(-1))\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, new_mem = batch\n",
    "        self.mem = None if new_mem[0] else self.mem\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        return {'val_loss': self.loss_fn(y_hat.view(-1,self.cfg.vocab_size), y.view(-1))}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         x, y, new_mem = batch\n",
    "#         self.mem = None if new_mem[0] else self.mem\n",
    "#         y_hat, self.mem = self.forward(x)\n",
    "#         y_pred = F.softmax(y_hat,dim=1).detach().cpu()\n",
    "#         return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred, 'y_true':y.cpu()}\n",
    "\n",
    "#     def test_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "#         y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "#         y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "#         roc_auc = weighted_auc(y_preds,y_trues, self.class_weights)\n",
    "        \n",
    "        tensorboard_logs = {'test_loss': avg_loss,'roc_auc':roc_auc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        \n",
    "        trn_ds = LMDataset(X_train,y_train,self.cfg.mem_len)\n",
    "        splr   = LMSampleR(trn_ds,self.bs,self.cfg.mem_len)\n",
    "        trn_dl = DataLoader(trn_ds, self.bs,sampler=splr,pin_memory=True)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = LMDataset(X_valid,y_valid,self.cfg.mem_len)\n",
    "        splr   = LMSampleR(vld_ds,self.bs,self.cfg.mem_len)\n",
    "        vld_dl = DataLoader(vld_ds,self.bs,sampler=splr,pin_memory=True)\n",
    "        return vld_dl\n",
    "    \n",
    "#     @pl.data_loader\n",
    "#     def test_dataloader(self):\n",
    "#         test = np.load('../data/Processed/test.npz')\n",
    "#         X_test = torch.tensor(test['arr_0'][:])\n",
    "#         y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "#         tst_ds = LMDataset(X_test,y_test,self.cfg.mem_len)\n",
    "#         splr   = LMSampleR(tst_ds,self.bs,self.cfg.mem_len)\n",
    "#         tst_dl = DataLoader(tst_ds,self.bs,sampler=splr,pin_memory=True)\n",
    "#         return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  25%|██▍       | 745/3000 [11:39<34:47,  1.08batch/s, batch_nb=744, gpu=0, loss=1.329, v_nb=14]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-672c6ba16de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      default_save_path='../data')    \n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m# ON CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/dp_mixin.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdp_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# RUN TRAIN STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mbatch_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_step_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;31m# nan grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36moptimizer_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mmodel_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                     \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                     \u001b[0;31m# track metrics for callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/root_module/hooks.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, use_amp, loss, optimizer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlnd/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp = Experiment(bs=32)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,max_nb_epochs=1, train_percent_check=0.01,val_percent_check=0.5,\n",
    "                     default_save_path='../data')    \n",
    "\n",
    "trainer.fit(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#add acc\n",
    "#dataloader slow when num_workers>0\n",
    "#add shuffler to samplers\n",
    "#apex https://github.com/adityaiitb/pyprof2\n",
    "#resnet+transformerxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet + TransformerXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        super(CustomDataset,self).__init__()\n",
    "        self.x, self.y = x,y\n",
    "        \n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i): return self.x[i].long(), self.y[i].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyclic lr with restart\n",
    "def lr_i(step_i,cycle_len = 1000, lrs = (3e-4,1e-5),warm_pct = 4/20):\n",
    "    cycle_i = step_i%cycle_len\n",
    "    warm_len = int(cycle_len*warm_pct)\n",
    "    cool_len = cycle_len - warm_len\n",
    "    lr_range = lrs[0]-lrs[1]\n",
    "    \n",
    "    if cycle_i < warm_len:\n",
    "        return lrs[1] + lr_range*(cycle_i/warm_len)\n",
    "    else:\n",
    "        return lrs[0] - lr_range*((cycle_i-warm_len)/cool_len)\n",
    "    \n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = np.linspace(0,50*30,10000)\n",
    "# y = list(map(lr_i,x))\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.ylim([0,4e-4])\n",
    "# plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,model,hparams):\n",
    "        super(Experiment, self).__init__()\n",
    "        self.model = model\n",
    "        self.bs = hparams['bs'] #batch size\n",
    "        self.lr = hparams['lr'] #batch size\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.class_weights = torch.load('../data/Processed/class_weights')\n",
    "        \n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "    def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "        # warm up lr\n",
    "        if self.trainer.global_step < 500:\n",
    "            lr_scale = min(1., float(self.trainer.global_step + 1) / 200.)\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr_scale * self.lr\n",
    "\n",
    "#         for pg in optimizer.param_groups:\n",
    "#             pg['lr'] = lr_i(self.trainer.global_step)\n",
    "            \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        y_pred = torch.sigmoid(y_hat)\n",
    "        return {'val_loss': self.loss_fn(y_hat, y),'y_pred':y_pred.cpu(), 'y_true':y.cpu()}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "\n",
    "        roc_auc = weighted_auc(y_preds.cuda(),y_trues.cuda(), self.class_weights.cuda())\n",
    "        acc = accuracy(y_preds,y_trues)\n",
    "        tensorboard_logs = {'val_loss': avg_loss,'valid_roc_auc':roc_auc,'valid_acc':acc}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, self.mem = self.forward(x)\n",
    "        y_pred = torch.sigmoid(y_hat)\n",
    "        return {'test_loss': self.loss_fn(y_hat, y),'y_pred':y_pred.cpu(), 'y_true':y.cpu()}\n",
    "\n",
    "    def test_end(self, outputs,save_preds=False):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        y_preds = torch.cat([x['y_pred'] for x in outputs])\n",
    "        y_trues = torch.cat([x['y_true'] for x in outputs]).byte()\n",
    "        if save_preds: self.preds=[y_preds,y_trues]\n",
    "            \n",
    "        roc_auc = weighted_auc(y_preds.cuda(),y_trues.cuda(), self.class_weights.cuda())\n",
    "        acc = accuracy(y_preds,y_trues)\n",
    "        tensorboard_logs = {'test_loss': avg_loss,'test_roc_auc':roc_auc,'test_acc':acc}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        print('Loading training dataset')\n",
    "        train_h5 = h5py.File('../data/Processed/train.hdf5')\n",
    "        X_train = torch.tensor(train_h5['X_train'][:])\n",
    "        y_train = torch.tensor(train_h5['y_train'][:])\n",
    "        train_h5.close()\n",
    "        \n",
    "        trn_ds = CustomDataset(X_train,y_train)\n",
    "        trn_dl = DataLoader(trn_ds, self.bs,pin_memory=True,shuffle=True)\n",
    "        return trn_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        valid = np.load('../data/Processed/valid.npz')\n",
    "        X_valid = torch.tensor(valid['arr_0'][:])\n",
    "        y_valid = torch.tensor(valid['arr_1'][:])\n",
    "\n",
    "        vld_ds = CustomDataset(X_valid,y_valid)\n",
    "        vld_dl = DataLoader(vld_ds,self.bs,pin_memory=True)\n",
    "        return vld_dl\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test = np.load('../data/Processed/test.npz')\n",
    "        X_test = torch.tensor(test['arr_0'][:])\n",
    "        y_test = torch.tensor(test['arr_1'][:])\n",
    "                              \n",
    "        tst_ds = CustomDataset(X_test,y_test)\n",
    "        tst_dl = DataLoader(tst_ds,self.bs,pin_memory=True)\n",
    "        return tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = ts.TransfoXLConfig(vocab_size=4, d_embed=8,d_model=256, n_head=4, d_head=16, d_inner=256, \n",
    "#                          n_layer=6, tgt_len=0, ext_len=0, mem_len=256, cutoffs=[1], )\n",
    "\n",
    "# model = models.ResTransXL(vocab_size=4, d_emb=64, tsfm_cfg=cfg, n_res_blocks=3, res_k=16, \n",
    "#                           skip_cnt=True, fc_h_dim=512, lin_p=0.5, WVN=True)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\t\t\t\t\n",
      "Resnet part:\t\t8894k\n",
      "Transformer-XL part:\t1988k\n",
      "Linear part:\t\t34028k\n",
      "Total:\t\t\t44911k\n"
     ]
    }
   ],
   "source": [
    "cfg = ts.TransfoXLConfig(vocab_size=4, d_embed=8,d_model=352, n_head=4, d_head=16, d_inner=256, \n",
    "                         n_layer=6, tgt_len=0, ext_len=0, mem_len=256, cutoffs=[1], )\n",
    "\n",
    "model = models.ResTransXL(vocab_size=4, d_emb=64, tsfm_cfg=cfg,skip_cnt=True,fc_h_dim=256,n_res_blocks=3, res_k=16\n",
    "                   ,LSTM=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(model,{'bs':25,'lr':1e-4})\n",
    "\n",
    "# chpt_path = '../data/lightning_logs/version_52/checkpoints/_ckpt_epoch_1.ckpt'\n",
    "# tags_csv = '../data/lightning_logs/version_7/meta_tags.csv'\n",
    "# exp.load_state_dict(torch.load(chpt_path)['state_dict'])\n",
    "# exp.load_from_checkpoint(chpt_path)\n",
    "# exp.load_from_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Loading training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   8%|▊         | 14116/179200 [12:53<2:15:44, 20.27batch/s, batch_nb=14115, gpu=0, loss=0.076, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   9%|▉         | 16199/179200 [14:44<2:25:15, 18.70batch/s, batch_nb=16198, gpu=0, loss=0.077, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|▉         | 17600/179200 [15:59<2:14:42, 19.99batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 1:  10%|▉         | 17610/179200 [16:00<1:42:46, 26.21batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17621/179200 [16:00<1:19:42, 33.78batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17632/179200 [16:00<1:03:35, 42.35batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17643/179200 [16:00<52:16, 51.50batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]  \n",
      "Epoch 1:  10%|▉         | 17654/179200 [16:00<44:19, 60.74batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17665/179200 [16:00<38:45, 69.47batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17676/179200 [16:00<34:51, 77.23batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17687/179200 [16:00<32:07, 83.79batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17698/179200 [16:00<30:15, 88.98batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17709/179200 [16:01<28:58, 92.91batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17720/179200 [16:01<28:05, 95.79batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17731/179200 [16:01<27:28, 97.97batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17742/179200 [16:01<27:02, 99.53batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17753/179200 [16:01<26:42, 100.74batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17764/179200 [16:01<26:30, 101.53batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17775/179200 [16:01<26:26, 101.75batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17786/179200 [16:01<26:23, 101.95batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17797/179200 [16:01<26:20, 102.11batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17808/179200 [16:01<26:15, 102.45batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17819/179200 [16:02<26:11, 102.72batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17830/179200 [16:02<26:07, 102.97batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17841/179200 [16:02<26:01, 103.32batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17852/179200 [16:02<25:57, 103.58batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17863/179200 [16:02<25:55, 103.73batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17874/179200 [16:02<25:53, 103.88batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17885/179200 [16:02<25:50, 104.02batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17896/179200 [16:02<25:49, 104.08batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|▉         | 17907/179200 [16:02<25:53, 103.84batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "Epoch 1:  10%|█         | 17920/179200 [16:04<25:55, 103.68batch/s, batch_nb=17599, gpu=0, loss=0.074, v_nb=55]\n",
      "                                                                 \u001b[A/home/ruben/anaconda3/envs/mlnd/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer_io.py:210: UserWarning: Did not find hyperparameters at model.hparams. Saving checkpoint without hyperparameters\n",
      "  \"Did not find hyperparameters at model.hparams. Saving checkpoint without\"\n",
      "Epoch 1:  11%|█         | 19227/179200 [17:17<2:27:29, 18.08batch/s, batch_nb=18906, gpu=0, loss=0.073, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  12%|█▏        | 22167/179200 [19:56<2:15:10, 19.36batch/s, batch_nb=21846, gpu=0, loss=0.074, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  14%|█▎        | 24227/179200 [21:46<2:22:26, 18.13batch/s, batch_nb=23906, gpu=0, loss=0.074, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|█▍        | 26411/179200 [23:45<2:12:17, 19.25batch/s, batch_nb=26090, gpu=0, loss=0.073, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  16%|█▌        | 28803/179200 [25:56<2:18:26, 18.11batch/s, batch_nb=28482, gpu=0, loss=0.074, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|█▋        | 30876/179200 [27:50<2:04:43, 19.82batch/s, batch_nb=30555, gpu=0, loss=0.073, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  18%|█▊        | 32899/179200 [29:41<2:11:48, 18.50batch/s, batch_nb=32578, gpu=0, loss=0.074, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▉        | 34903/179200 [31:32<2:17:00, 17.55batch/s, batch_nb=34582, gpu=0, loss=0.073, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  20%|█▉        | 35520/179200 [32:07<2:14:24, 17.82batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35530/179200 [32:07<1:41:17, 23.64batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35541/179200 [32:07<1:17:54, 30.73batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35551/179200 [32:07<1:01:44, 38.77batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35562/179200 [32:07<50:04, 47.81batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]  \n",
      "Epoch 1:  20%|█▉        | 35573/179200 [32:07<41:54, 57.13batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35584/179200 [32:07<36:12, 66.10batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35595/179200 [32:07<32:12, 74.33batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35606/179200 [32:07<29:23, 81.44batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35617/179200 [32:07<27:25, 87.25batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35628/179200 [32:08<26:04, 91.76batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35639/179200 [32:08<25:07, 95.25batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35650/179200 [32:08<24:27, 97.85batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35661/179200 [32:08<23:57, 99.84batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35672/179200 [32:08<23:38, 101.20batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35683/179200 [32:08<23:23, 102.26batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35694/179200 [32:08<23:30, 101.73batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35705/179200 [32:08<23:18, 102.63batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35716/179200 [32:08<23:09, 103.24batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35727/179200 [32:09<23:02, 103.74batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35738/179200 [32:09<22:59, 103.99batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35749/179200 [32:09<22:56, 104.22batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35760/179200 [32:09<22:54, 104.37batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35771/179200 [32:09<22:54, 104.39batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35782/179200 [32:09<22:52, 104.47batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35793/179200 [32:09<22:52, 104.49batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35804/179200 [32:09<23:00, 103.91batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35815/179200 [32:09<22:56, 104.17batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35826/179200 [32:09<22:53, 104.36batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|█▉        | 35837/179200 [32:10<22:52, 104.44batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  20%|██        | 35840/179200 [32:11<22:52, 104.44batch/s, batch_nb=35199, gpu=0, loss=0.073, v_nb=55]\n",
      "Epoch 1:  21%|██▏       | 38515/179200 [34:42<2:16:56, 17.12batch/s, batch_nb=37874, gpu=0, loss=0.072, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  23%|██▎       | 41419/179200 [37:24<2:06:43, 18.12batch/s, batch_nb=40778, gpu=0, loss=0.071, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  25%|██▍       | 44183/179200 [39:57<2:11:45, 17.08batch/s, batch_nb=43542, gpu=0, loss=0.072, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  27%|██▋       | 48199/179200 [43:49<2:04:48, 17.49batch/s, batch_nb=47558, gpu=0, loss=0.070, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  28%|██▊       | 49395/179200 [45:00<2:15:38, 15.95batch/s, batch_nb=48754, gpu=0, loss=0.072, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  30%|██▉       | 53440/179200 [49:02<2:02:48, 17.07batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53443/179200 [49:02<1:42:20, 20.48batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53453/179200 [49:03<1:18:21, 26.75batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53463/179200 [49:03<1:01:32, 34.05batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53473/179200 [49:03<49:33, 42.28batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]  \n",
      "Epoch 1:  30%|██▉       | 53483/179200 [49:03<41:13, 50.82batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53493/179200 [49:03<35:32, 58.94batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53503/179200 [49:03<31:20, 66.83batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53513/179200 [49:03<28:29, 73.54batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53523/179200 [49:03<26:25, 79.26batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53533/179200 [49:03<25:35, 81.83batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53543/179200 [49:04<24:54, 84.10batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53553/179200 [49:04<24:21, 85.97batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53563/179200 [49:04<24:01, 87.15batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53573/179200 [49:04<23:50, 87.79batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53583/179200 [49:04<23:32, 88.91batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53593/179200 [49:04<23:32, 88.95batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53603/179200 [49:04<23:25, 89.36batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53613/179200 [49:04<23:28, 89.19batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53623/179200 [49:04<23:20, 89.68batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53633/179200 [49:05<23:16, 89.90batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53643/179200 [49:05<23:13, 90.13batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53653/179200 [49:05<23:18, 89.78batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53663/179200 [49:05<23:07, 90.50batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53673/179200 [49:05<22:32, 92.78batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53683/179200 [49:05<22:35, 92.57batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53693/179200 [49:05<23:00, 90.90batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53703/179200 [49:05<23:03, 90.68batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53713/179200 [49:05<23:02, 90.77batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53723/179200 [49:06<23:13, 90.03batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53733/179200 [49:06<23:05, 90.54batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53743/179200 [49:06<23:15, 89.88batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|██▉       | 53752/179200 [49:06<23:51, 87.66batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|███       | 53760/179200 [49:07<23:50, 87.66batch/s, batch_nb=52799, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  30%|███       | 53771/179200 [49:11<4:40:02,  7.46batch/s, batch_nb=52810, gpu=0, loss=0.070, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  30%|███       | 53911/179200 [49:20<2:03:50, 16.86batch/s, batch_nb=52950, gpu=0, loss=0.070, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  31%|███▏      | 56239/179200 [51:48<2:04:25, 16.47batch/s, batch_nb=55278, gpu=0, loss=0.071, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 58407/179200 [54:08<2:04:07, 16.22batch/s, batch_nb=57446, gpu=0, loss=0.070, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  35%|███▍      | 62439/179200 [58:29<2:02:31, 15.88batch/s, batch_nb=61478, gpu=0, loss=0.071, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  36%|███▌      | 64459/179200 [1:00:31<1:43:51, 18.41batch/s, batch_nb=63498, gpu=0, loss=0.070, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  37%|███▋      | 66561/179200 [1:02:33<1:50:18, 17.02batch/s, batch_nb=65600, gpu=0, loss=0.069, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  38%|███▊      | 68135/179200 [1:04:04<1:45:42, 17.51batch/s, batch_nb=67174, gpu=0, loss=0.071, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  40%|███▉      | 71360/179200 [1:07:12<1:42:29, 17.54batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71364/179200 [1:07:13<1:23:27, 21.54batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71375/179200 [1:07:13<1:03:41, 28.21batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71385/179200 [1:07:13<50:15, 35.75batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]  \n",
      "Epoch 1:  40%|███▉      | 71396/179200 [1:07:13<40:26, 44.42batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71407/179200 [1:07:13<33:34, 53.51batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71418/179200 [1:07:13<28:45, 62.47batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71429/179200 [1:07:13<25:24, 70.70batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71440/179200 [1:07:13<23:01, 77.99batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71451/179200 [1:07:13<21:18, 84.26batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71461/179200 [1:07:13<20:31, 87.46batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71472/179200 [1:07:14<19:41, 91.21batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71483/179200 [1:07:14<19:05, 94.04batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71494/179200 [1:07:14<18:40, 96.16batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71505/179200 [1:07:14<18:22, 97.71batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71516/179200 [1:07:14<18:09, 98.87batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71527/179200 [1:07:14<18:00, 99.64batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71538/179200 [1:07:14<17:53, 100.27batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71549/179200 [1:07:14<17:49, 100.61batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71560/179200 [1:07:14<17:46, 100.93batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71571/179200 [1:07:15<17:45, 101.06batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71582/179200 [1:07:15<17:53, 100.22batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71593/179200 [1:07:15<17:47, 100.80batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71604/179200 [1:07:15<17:40, 101.41batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71615/179200 [1:07:15<17:35, 101.95batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71626/179200 [1:07:15<17:32, 102.24batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71637/179200 [1:07:15<17:30, 102.35batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71648/179200 [1:07:15<17:29, 102.50batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71659/179200 [1:07:15<17:26, 102.75batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|███▉      | 71670/179200 [1:07:16<17:23, 103.07batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|████      | 71680/179200 [1:07:17<17:23, 103.07batch/s, batch_nb=70399, gpu=0, loss=0.070, v_nb=55]\n",
      "Epoch 1:  40%|████      | 72275/179200 [1:07:56<1:37:13, 18.33batch/s, batch_nb=70994, gpu=0, loss=0.071, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  43%|████▎     | 76303/179200 [1:11:50<1:41:19, 16.92batch/s, batch_nb=75022, gpu=0, loss=0.069, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  44%|████▎     | 78335/179200 [1:13:49<1:35:23, 17.62batch/s, batch_nb=77054, gpu=0, loss=0.070, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  44%|████▍     | 79422/179200 [1:14:53<1:42:01, 16.30batch/s, batch_nb=78141, gpu=0, loss=0.072, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  46%|████▌     | 81975/179200 [1:17:23<1:40:07, 16.19batch/s, batch_nb=80694, gpu=0, loss=0.070, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  48%|████▊     | 85979/179200 [1:21:20<1:26:49, 17.89batch/s, batch_nb=84698, gpu=0, loss=0.069, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  49%|████▉     | 88119/179200 [1:23:30<1:24:25, 17.98batch/s, batch_nb=86838, gpu=0, loss=0.070, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|████▉     | 89280/179200 [1:24:42<1:26:39, 17.29batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89284/179200 [1:24:43<1:10:06, 21.38batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89295/179200 [1:24:43<53:23, 28.07batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]  \n",
      "Epoch 1:  50%|████▉     | 89305/179200 [1:24:43<41:54, 35.75batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89315/179200 [1:24:43<33:55, 44.16batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89326/179200 [1:24:43<28:09, 53.19batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89337/179200 [1:24:43<24:08, 62.05batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89348/179200 [1:24:43<21:15, 70.46batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89359/179200 [1:24:43<19:17, 77.60batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89370/179200 [1:24:43<17:52, 83.75batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89381/179200 [1:24:43<16:58, 88.21batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89392/179200 [1:24:44<16:14, 92.14batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89403/179200 [1:24:44<15:38, 95.66batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89414/179200 [1:24:44<15:13, 98.33batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89425/179200 [1:24:44<14:55, 100.22batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89436/179200 [1:24:44<14:43, 101.62batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89447/179200 [1:24:44<14:34, 102.67batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89458/179200 [1:24:44<14:31, 102.98batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89469/179200 [1:24:44<14:32, 102.87batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89480/179200 [1:24:44<14:45, 101.36batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89491/179200 [1:24:45<14:41, 101.78batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89502/179200 [1:24:45<14:52, 100.45batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89513/179200 [1:24:45<15:03, 99.28batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55] \n",
      "Epoch 1:  50%|████▉     | 89523/179200 [1:24:45<15:30, 96.33batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89534/179200 [1:24:45<15:09, 98.63batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89545/179200 [1:24:45<14:54, 100.23batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89556/179200 [1:24:45<14:43, 101.50batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89567/179200 [1:24:45<14:34, 102.51batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89578/179200 [1:24:45<14:39, 101.93batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|████▉     | 89589/179200 [1:24:46<14:38, 102.02batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  50%|█████     | 89600/179200 [1:24:47<14:30, 102.92batch/s, batch_nb=87999, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  51%|█████     | 90579/179200 [1:25:52<1:24:17, 17.52batch/s, batch_nb=88978, gpu=0, loss=0.069, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  52%|█████▏    | 92588/179200 [1:27:56<1:30:21, 15.98batch/s, batch_nb=90987, gpu=0, loss=0.069, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  53%|█████▎    | 94601/179200 [1:29:59<1:32:45, 15.20batch/s, batch_nb=93000, gpu=0, loss=0.068, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  53%|█████▎    | 94727/179200 [1:30:07<1:25:44, 16.42batch/s, batch_nb=93126, gpu=0, loss=0.069, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  54%|█████▍    | 96919/179200 [1:32:26<1:25:35, 16.02batch/s, batch_nb=95318, gpu=0, loss=0.070, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  56%|█████▋    | 101011/179200 [1:36:46<1:17:20, 16.85batch/s, batch_nb=99410, gpu=0, loss=0.070, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  58%|█████▊    | 103075/179200 [1:38:56<1:22:29, 15.38batch/s, batch_nb=101474, gpu=0, loss=0.069, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  58%|█████▊    | 103559/179200 [1:39:28<1:15:02, 16.80batch/s, batch_nb=101958, gpu=0, loss=0.070, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  60%|█████▉    | 107200/179200 [1:43:19<1:06:32, 18.03batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107205/179200 [1:43:19<53:00, 22.64batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]  \n",
      "Epoch 1:  60%|█████▉    | 107216/179200 [1:43:19<40:40, 29.50batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107226/179200 [1:43:19<32:06, 37.36batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107237/179200 [1:43:19<25:57, 46.21batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107248/179200 [1:43:19<21:37, 55.44batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107259/179200 [1:43:20<18:36, 64.43batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107270/179200 [1:43:20<16:34, 72.29batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107281/179200 [1:43:20<15:04, 79.53batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107292/179200 [1:43:20<14:01, 85.40batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107303/179200 [1:43:20<13:18, 90.05batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107314/179200 [1:43:20<12:47, 93.64batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107325/179200 [1:43:20<12:28, 96.06batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107336/179200 [1:43:20<12:11, 98.21batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107347/179200 [1:43:20<12:00, 99.76batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107358/179200 [1:43:20<11:51, 101.03batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107369/179200 [1:43:21<11:46, 101.63batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107380/179200 [1:43:21<11:40, 102.59batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107391/179200 [1:43:21<11:35, 103.24batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107402/179200 [1:43:21<11:32, 103.62batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107413/179200 [1:43:21<11:30, 103.96batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107424/179200 [1:43:21<11:31, 103.75batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107435/179200 [1:43:21<11:30, 103.87batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107446/179200 [1:43:21<11:30, 103.94batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107457/179200 [1:43:21<11:31, 103.82batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107468/179200 [1:43:22<11:30, 103.86batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107479/179200 [1:43:22<11:31, 103.66batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107490/179200 [1:43:22<11:29, 104.07batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107501/179200 [1:43:22<11:27, 104.31batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|█████▉    | 107512/179200 [1:43:22<11:26, 104.39batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|██████    | 107520/179200 [1:43:23<11:26, 104.39batch/s, batch_nb=105599, gpu=0, loss=0.069, v_nb=55]\n",
      "Epoch 1:  60%|██████    | 107979/179200 [1:43:55<1:08:57, 17.21batch/s, batch_nb=106058, gpu=0, loss=0.068, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  61%|██████▏   | 110151/179200 [1:46:16<1:05:16, 17.63batch/s, batch_nb=108230, gpu=0, loss=0.068, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  63%|██████▎   | 112182/179200 [1:48:27<1:14:57, 14.90batch/s, batch_nb=110261, gpu=0, loss=0.068, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  63%|██████▎   | 112662/179200 [1:48:59<1:19:43, 13.91batch/s, batch_nb=110741, gpu=0, loss=0.068, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  65%|██████▌   | 116695/179200 [1:53:25<1:09:17, 15.03batch/s, batch_nb=114774, gpu=0, loss=0.068, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  65%|██████▌   | 116921/179200 [1:53:40<1:01:36, 16.85batch/s, batch_nb=115000, gpu=0, loss=0.069, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  68%|██████▊   | 121055/179200 [1:58:15<1:05:34, 14.78batch/s, batch_nb=119134, gpu=0, loss=0.068, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  69%|██████▊   | 123131/179200 [2:00:29<55:58, 16.70batch/s, batch_nb=121210, gpu=0, loss=0.069, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  70%|██████▉   | 125120/179200 [2:02:32<51:51, 17.38batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]  \n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 1:  70%|██████▉   | 125130/179200 [2:02:32<39:08, 23.02batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125141/179200 [2:02:32<29:58, 30.05batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125152/179200 [2:02:33<23:39, 38.07batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125162/179200 [2:02:33<19:16, 46.74batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125173/179200 [2:02:33<16:05, 55.95batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125184/179200 [2:02:33<13:51, 64.95batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125195/179200 [2:02:33<12:17, 73.19batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125206/179200 [2:02:33<11:12, 80.34batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125217/179200 [2:02:33<10:26, 86.21batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125228/179200 [2:02:33<10:00, 89.83batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125239/179200 [2:02:33<09:36, 93.64batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125250/179200 [2:02:33<09:19, 96.50batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125261/179200 [2:02:34<09:07, 98.57batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125272/179200 [2:02:34<08:58, 100.15batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125283/179200 [2:02:34<08:55, 100.64batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125294/179200 [2:02:34<08:50, 101.56batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125305/179200 [2:02:34<08:49, 101.72batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125316/179200 [2:02:34<08:47, 102.20batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125327/179200 [2:02:34<08:46, 102.36batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125338/179200 [2:02:34<08:45, 102.57batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125349/179200 [2:02:34<08:49, 101.66batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125360/179200 [2:02:35<08:47, 102.14batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125371/179200 [2:02:35<08:44, 102.55batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125382/179200 [2:02:35<08:43, 102.78batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125393/179200 [2:02:35<08:42, 103.01batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125404/179200 [2:02:35<08:41, 103.18batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125415/179200 [2:02:35<08:50, 101.39batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|██████▉   | 125426/179200 [2:02:35<08:47, 101.95batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|███████   | 125440/179200 [2:02:37<08:44, 102.46batch/s, batch_nb=123199, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1:  70%|███████   | 125539/179200 [2:02:46<52:19, 17.09batch/s, batch_nb=123298, gpu=0, loss=0.066, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  71%|███████   | 127662/179200 [2:04:58<56:37, 15.17batch/s, batch_nb=125421, gpu=0, loss=0.068, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  72%|███████▏  | 129602/179200 [2:06:58<51:18, 16.11batch/s, batch_nb=127361, gpu=0, loss=0.066, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  74%|███████▍  | 132875/179200 [2:10:24<47:20, 16.31batch/s, batch_nb=130634, gpu=0, loss=0.068, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  77%|███████▋  | 137211/179200 [2:15:02<39:55, 17.53batch/s, batch_nb=134970, gpu=0, loss=0.068, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  78%|███████▊  | 139387/179200 [2:17:20<40:43, 16.30batch/s, batch_nb=137146, gpu=0, loss=0.067, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  79%|███████▊  | 140761/179200 [2:18:48<36:37, 17.49batch/s, batch_nb=138520, gpu=0, loss=0.068, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|███████▉  | 143040/179200 [2:21:19<35:47, 16.84batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 1:  80%|███████▉  | 143050/179200 [2:21:19<27:02, 22.28batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143060/179200 [2:21:19<20:54, 28.81batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143070/179200 [2:21:19<16:30, 36.48batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143080/179200 [2:21:19<13:23, 44.95batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143090/179200 [2:21:19<11:15, 53.43batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143100/179200 [2:21:19<09:44, 61.74batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143110/179200 [2:21:20<08:41, 69.26batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143120/179200 [2:21:20<07:56, 75.78batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143130/179200 [2:21:20<07:24, 81.15batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143140/179200 [2:21:20<07:02, 85.29batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143150/179200 [2:21:20<06:46, 88.59batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143160/179200 [2:21:20<06:35, 91.05batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143170/179200 [2:21:20<06:31, 92.10batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143180/179200 [2:21:20<06:24, 93.57batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143190/179200 [2:21:20<06:20, 94.53batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143200/179200 [2:21:21<06:17, 95.46batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143210/179200 [2:21:21<06:16, 95.63batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143220/179200 [2:21:21<06:15, 95.85batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143230/179200 [2:21:21<06:17, 95.41batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143240/179200 [2:21:21<06:16, 95.53batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143250/179200 [2:21:21<06:15, 95.81batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143260/179200 [2:21:21<06:14, 96.06batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143270/179200 [2:21:21<06:14, 95.87batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143280/179200 [2:21:21<06:14, 95.86batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143290/179200 [2:21:21<06:13, 96.08batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143300/179200 [2:21:22<06:12, 96.33batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143310/179200 [2:21:22<06:13, 96.14batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143320/179200 [2:21:22<06:13, 96.00batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143330/179200 [2:21:22<06:12, 96.39batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143340/179200 [2:21:22<06:11, 96.63batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|███████▉  | 143350/179200 [2:21:22<06:09, 96.97batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  80%|████████  | 143360/179200 [2:21:23<06:09, 97.00batch/s, batch_nb=140799, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  81%|████████  | 145301/179200 [2:23:31<34:44, 16.27batch/s, batch_nb=142740, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  82%|████████▏ | 146351/179200 [2:24:39<32:42, 16.74batch/s, batch_nb=143790, gpu=0, loss=0.067, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  83%|████████▎ | 148815/179200 [2:27:17<32:49, 15.42batch/s, batch_nb=146254, gpu=0, loss=0.067, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  85%|████████▍ | 151439/179200 [2:30:07<27:07, 17.06batch/s, batch_nb=148878, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  86%|████████▌ | 153661/179200 [2:32:32<24:47, 17.17batch/s, batch_nb=151100, gpu=0, loss=0.067, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  88%|████████▊ | 158081/179200 [2:37:34<21:27, 16.40batch/s, batch_nb=155520, gpu=0, loss=0.067, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  89%|████████▉ | 160111/179200 [2:39:51<19:13, 16.54batch/s, batch_nb=157550, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|████████▉ | 160960/179200 [2:40:49<19:20, 15.72batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 160963/179200 [2:40:49<15:59, 19.01batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 160973/179200 [2:40:49<12:11, 24.92batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 160982/179200 [2:40:49<09:36, 31.63batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 160991/179200 [2:40:49<07:44, 39.23batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161000/179200 [2:40:49<06:30, 46.58batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161010/179200 [2:40:49<05:33, 54.60batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161019/179200 [2:40:49<04:54, 61.83batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161029/179200 [2:40:49<04:25, 68.37batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161038/179200 [2:40:50<04:08, 73.01batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161047/179200 [2:40:50<03:56, 76.89batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161057/179200 [2:40:50<03:45, 80.49batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161067/179200 [2:40:50<03:36, 83.72batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161076/179200 [2:40:50<03:33, 85.07batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161086/179200 [2:40:50<03:29, 86.60batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161096/179200 [2:40:50<03:26, 87.76batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161105/179200 [2:40:50<03:25, 87.95batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161114/179200 [2:40:50<03:25, 88.21batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161124/179200 [2:40:51<03:20, 89.95batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161134/179200 [2:40:51<03:18, 90.99batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161144/179200 [2:40:51<03:15, 92.38batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161154/179200 [2:40:51<03:14, 92.72batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161164/179200 [2:40:51<03:13, 93.44batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161174/179200 [2:40:51<03:11, 93.95batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161184/179200 [2:40:51<03:12, 93.58batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161194/179200 [2:40:51<03:19, 90.13batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161204/179200 [2:40:51<03:18, 90.44batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161214/179200 [2:40:52<03:20, 89.56batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161223/179200 [2:40:52<03:23, 88.37batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161233/179200 [2:40:52<03:20, 89.42batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161243/179200 [2:40:52<03:19, 90.12batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161253/179200 [2:40:52<03:16, 91.44batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161263/179200 [2:40:52<03:16, 91.22batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|████████▉ | 161273/179200 [2:40:52<03:17, 90.91batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  90%|█████████ | 161280/179200 [2:40:54<03:17, 90.91batch/s, batch_nb=158399, gpu=0, loss=0.068, v_nb=55]\n",
      "Epoch 1:  91%|█████████ | 162631/179200 [2:42:26<16:26, 16.79batch/s, batch_nb=159750, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  92%|█████████▏| 164887/179200 [2:44:58<16:56, 14.08batch/s, batch_nb=162006, gpu=0, loss=0.068, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  93%|█████████▎| 166891/179200 [2:47:13<13:06, 15.65batch/s, batch_nb=164010, gpu=0, loss=0.067, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  93%|█████████▎| 167311/179200 [2:47:40<11:41, 16.96batch/s, batch_nb=164430, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  96%|█████████▌| 171521/179200 [2:52:18<07:08, 17.91batch/s, batch_nb=168640, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  97%|█████████▋| 173521/179200 [2:54:30<05:39, 16.74batch/s, batch_nb=170640, gpu=0, loss=0.067, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 177581/179200 [2:59:01<02:07, 12.68batch/s, batch_nb=174700, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████▉| 178880/179200 [3:00:28<00:18, 16.95batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 178890/179200 [3:00:28<00:13, 22.57batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 178900/179200 [3:00:28<00:10, 29.39batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 178911/179200 [3:00:28<00:07, 37.43batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 178922/179200 [3:00:28<00:06, 46.30batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 178933/179200 [3:00:28<00:04, 55.53batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 178944/179200 [3:00:28<00:03, 64.53batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 178955/179200 [3:00:28<00:03, 72.83batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 178966/179200 [3:00:28<00:02, 79.44batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 178977/179200 [3:00:29<00:02, 85.54batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 178988/179200 [3:00:29<00:02, 90.35batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 178999/179200 [3:00:29<00:02, 94.05batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179010/179200 [3:00:29<00:01, 96.78batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179021/179200 [3:00:29<00:01, 98.66batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179032/179200 [3:00:29<00:01, 99.95batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179043/179200 [3:00:29<00:01, 101.42batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179054/179200 [3:00:29<00:01, 102.40batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179065/179200 [3:00:29<00:01, 102.93batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179076/179200 [3:00:30<00:01, 103.02batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179087/179200 [3:00:30<00:01, 102.35batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179098/179200 [3:00:30<00:00, 102.75batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179109/179200 [3:00:30<00:00, 103.12batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179120/179200 [3:00:30<00:00, 102.02batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179131/179200 [3:00:30<00:00, 102.58batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179142/179200 [3:00:30<00:00, 102.97batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179153/179200 [3:00:30<00:00, 102.07batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179164/179200 [3:00:30<00:00, 102.61batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179175/179200 [3:00:30<00:00, 103.01batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179186/179200 [3:00:31<00:00, 103.34batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|█████████▉| 179197/179200 [3:00:31<00:00, 103.56batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 1: 100%|██████████| 179200/179200 [3:00:32<00:00, 103.56batch/s, batch_nb=175999, gpu=0, loss=0.067, v_nb=55]\n",
      "Epoch 2:   1%|          | 1215/179200 [01:21<3:25:44, 14.42batch/s, batch_nb=1214, gpu=0, loss=0.066, v_nb=55]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   2%|▏         | 3535/179200 [03:59<3:29:57, 13.94batch/s, batch_nb=3534, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   3%|▎         | 5541/179200 [06:19<3:10:57, 15.16batch/s, batch_nb=5540, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   4%|▍         | 6819/179200 [07:52<3:04:10, 15.60batch/s, batch_nb=6818, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   6%|▋         | 11395/179200 [13:19<3:22:42, 13.80batch/s, batch_nb=11394, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   8%|▊         | 13951/179200 [16:15<2:46:05, 16.58batch/s, batch_nb=13950, gpu=0, loss=0.067, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   8%|▊         | 15051/179200 [17:30<2:45:23, 16.54batch/s, batch_nb=15050, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  10%|▉         | 17600/179200 [20:26<2:48:54, 15.95batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 2:  10%|▉         | 17609/179200 [20:27<2:07:46, 21.08batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17618/179200 [20:27<1:38:37, 27.31batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17628/179200 [20:27<1:18:00, 34.52batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17637/179200 [20:27<1:03:43, 42.26batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17646/179200 [20:27<54:22, 49.53batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]  \n",
      "Epoch 2:  10%|▉         | 17655/179200 [20:27<47:05, 57.17batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17664/179200 [20:27<42:00, 64.09batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17675/179200 [20:27<37:07, 72.50batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17686/179200 [20:27<33:41, 79.89batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17696/179200 [20:28<32:25, 83.01batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17706/179200 [20:28<31:15, 86.09batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17716/179200 [20:28<30:52, 87.15batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17726/179200 [20:28<30:12, 89.07batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17737/179200 [20:28<29:02, 92.68batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17748/179200 [20:28<28:07, 95.67batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17759/179200 [20:28<27:29, 97.87batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17770/179200 [20:28<27:18, 98.54batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17781/179200 [20:28<26:52, 100.12batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17792/179200 [20:28<26:40, 100.87batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17803/179200 [20:29<26:48, 100.34batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17814/179200 [20:29<26:42, 100.73batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17825/179200 [20:29<26:36, 101.06batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17836/179200 [20:29<26:32, 101.36batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17847/179200 [20:29<26:27, 101.62batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17858/179200 [20:29<26:44, 100.58batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17869/179200 [20:29<26:31, 101.38batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17880/179200 [20:29<26:22, 101.96batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17891/179200 [20:29<26:15, 102.38batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|▉         | 17902/179200 [20:30<26:10, 102.71batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|█         | 17920/179200 [20:31<26:07, 102.87batch/s, batch_nb=17599, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  10%|█         | 18807/179200 [21:37<2:52:02, 15.54batch/s, batch_nb=18486, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  13%|█▎        | 22941/179200 [26:46<2:52:50, 15.07batch/s, batch_nb=22620, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  14%|█▍        | 25435/179200 [29:54<3:18:42, 12.90batch/s, batch_nb=25114, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  16%|█▌        | 28281/179200 [33:30<2:36:53, 16.03batch/s, batch_nb=27960, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  17%|█▋        | 30487/179200 [36:14<2:46:37, 14.88batch/s, batch_nb=30166, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  18%|█▊        | 32895/179200 [39:08<2:46:14, 14.67batch/s, batch_nb=32574, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  18%|█▊        | 32951/179200 [39:12<2:25:40, 16.73batch/s, batch_nb=32630, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  20%|█▉        | 35520/179200 [42:15<2:32:47, 15.67batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35529/179200 [42:15<1:55:02, 20.82batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35539/179200 [42:15<1:28:01, 27.20batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35549/179200 [42:15<1:09:18, 34.55batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35559/179200 [42:15<56:10, 42.62batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]  \n",
      "Epoch 2:  20%|█▉        | 35569/179200 [42:15<46:53, 51.05batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35579/179200 [42:15<40:15, 59.46batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35589/179200 [42:15<35:37, 67.19batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35599/179200 [42:15<32:22, 73.91batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35609/179200 [42:16<30:05, 79.54batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35619/179200 [42:16<28:43, 83.29batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35629/179200 [42:16<27:39, 86.54batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35639/179200 [42:16<26:45, 89.39batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35649/179200 [42:16<26:18, 90.95batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35659/179200 [42:16<25:56, 92.21batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35669/179200 [42:16<25:32, 93.68batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35679/179200 [42:16<25:24, 94.16batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35689/179200 [42:16<25:15, 94.66batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35699/179200 [42:16<25:16, 94.63batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35709/179200 [42:17<25:08, 95.14batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35719/179200 [42:17<25:10, 94.97batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35729/179200 [42:17<25:12, 94.83batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35739/179200 [42:17<25:25, 94.07batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35749/179200 [42:17<25:24, 94.09batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35759/179200 [42:17<25:14, 94.68batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35769/179200 [42:17<25:09, 95.02batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35779/179200 [42:17<25:09, 95.02batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35789/179200 [42:17<25:19, 94.39batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35799/179200 [42:18<25:16, 94.55batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35809/179200 [42:18<25:39, 93.14batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35819/179200 [42:18<25:26, 93.90batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35829/179200 [42:18<25:18, 94.39batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|█▉        | 35839/179200 [42:18<25:10, 94.94batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  20%|██        | 35840/179200 [42:19<25:10, 94.94batch/s, batch_nb=35199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  21%|██        | 37639/179200 [44:34<2:24:35, 16.32batch/s, batch_nb=36998, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  22%|██▏       | 39640/179200 [46:58<2:19:07, 16.72batch/s, batch_nb=38999, gpu=0, loss=0.067, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  23%|██▎       | 40911/179200 [48:31<2:14:55, 17.08batch/s, batch_nb=40270, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  25%|██▌       | 44915/179200 [53:25<2:28:34, 15.06batch/s, batch_nb=44274, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  26%|██▌       | 46671/179200 [55:30<2:13:07, 16.59batch/s, batch_nb=46030, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  28%|██▊       | 50855/179200 [1:00:29<2:53:09, 12.35batch/s, batch_nb=50214, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  30%|██▉       | 53411/179200 [1:03:43<2:06:51, 16.53batch/s, batch_nb=52770, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  30%|██▉       | 53440/179200 [1:03:45<2:17:51, 15.20batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53443/179200 [1:03:45<1:53:59, 18.39batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53453/179200 [1:03:45<1:26:35, 24.20batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53462/179200 [1:03:45<1:07:53, 30.87batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53471/179200 [1:03:45<55:07, 38.01batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]  \n",
      "Epoch 2:  30%|██▉       | 53481/179200 [1:03:45<45:20, 46.21batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53492/179200 [1:03:45<37:53, 55.30batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53502/179200 [1:03:45<32:55, 63.63batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53512/179200 [1:03:45<29:26, 71.15batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53523/179200 [1:03:45<26:44, 78.31batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53534/179200 [1:03:46<25:05, 83.46batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53544/179200 [1:03:46<24:35, 85.17batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53554/179200 [1:03:46<25:03, 83.55batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53564/179200 [1:03:46<24:07, 86.81batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53575/179200 [1:03:46<22:57, 91.21batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53586/179200 [1:03:46<22:08, 94.56batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53597/179200 [1:03:46<21:35, 96.96batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53608/179200 [1:03:46<21:11, 98.79batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53619/179200 [1:03:46<21:06, 99.16batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53630/179200 [1:03:47<21:04, 99.27batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53641/179200 [1:03:47<20:49, 100.52batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53652/179200 [1:03:47<20:36, 101.53batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53663/179200 [1:03:47<20:28, 102.18batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53674/179200 [1:03:47<20:23, 102.64batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53685/179200 [1:03:47<20:30, 101.97batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53696/179200 [1:03:47<20:49, 100.43batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53707/179200 [1:03:47<20:48, 100.54batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53718/179200 [1:03:47<21:10, 98.74batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55] \n",
      "Epoch 2:  30%|██▉       | 53728/179200 [1:03:48<22:05, 94.63batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53738/179200 [1:03:48<22:44, 91.94batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|██▉       | 53748/179200 [1:03:48<22:16, 93.90batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  30%|███       | 53760/179200 [1:03:49<21:46, 96.04batch/s, batch_nb=52799, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2:  31%|███▏      | 56008/179200 [1:06:38<2:26:59, 13.97batch/s, batch_nb=55047, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  33%|███▎      | 58487/179200 [1:09:54<2:29:48, 13.43batch/s, batch_nb=57526, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  34%|███▍      | 60519/179200 [1:12:37<2:19:35, 14.17batch/s, batch_nb=59558, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  35%|███▌      | 62721/179200 [1:15:35<2:12:41, 14.63batch/s, batch_nb=61760, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  36%|███▋      | 65281/179200 [1:19:03<2:11:11, 14.47batch/s, batch_nb=64320, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  38%|███▊      | 67391/179200 [1:21:55<2:05:00, 14.91batch/s, batch_nb=66430, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  39%|███▉      | 69475/179200 [1:24:45<2:37:04, 11.64batch/s, batch_nb=68514, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  40%|███▉      | 71360/179200 [1:27:19<2:09:26, 13.89batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71363/179200 [1:27:19<1:45:15, 17.07batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71372/179200 [1:27:19<1:19:59, 22.46batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71381/179200 [1:27:19<1:02:02, 28.96batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71390/179200 [1:27:19<49:42, 36.15batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]  \n",
      "Epoch 2:  40%|███▉      | 71400/179200 [1:27:19<40:43, 44.12batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71409/179200 [1:27:19<34:30, 52.06batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71418/179200 [1:27:20<30:10, 59.54batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71427/179200 [1:27:20<27:08, 66.19batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71436/179200 [1:27:20<25:15, 71.12batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71445/179200 [1:27:20<23:58, 74.89batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71454/179200 [1:27:20<22:51, 78.55batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71464/179200 [1:27:20<21:56, 81.86batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71473/179200 [1:27:20<21:44, 82.57batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71482/179200 [1:27:20<21:23, 83.92batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71491/179200 [1:27:20<21:17, 84.30batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71500/179200 [1:27:21<21:11, 84.67batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71509/179200 [1:27:21<20:58, 85.55batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71518/179200 [1:27:21<21:04, 85.18batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71527/179200 [1:27:21<20:57, 85.63batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71536/179200 [1:27:21<20:44, 86.52batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71545/179200 [1:27:21<20:54, 85.81batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71554/179200 [1:27:21<20:44, 86.50batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71563/179200 [1:27:21<20:42, 86.65batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71572/179200 [1:27:21<20:34, 87.19batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71581/179200 [1:27:21<20:24, 87.86batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71590/179200 [1:27:22<20:35, 87.11batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71599/179200 [1:27:22<20:54, 85.76batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71608/179200 [1:27:22<20:44, 86.44batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71617/179200 [1:27:22<20:35, 87.07batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71626/179200 [1:27:22<20:48, 86.18batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71635/179200 [1:27:22<20:43, 86.52batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71644/179200 [1:27:22<20:32, 87.24batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71653/179200 [1:27:22<20:37, 86.94batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71662/179200 [1:27:22<20:31, 87.29batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|███▉      | 71671/179200 [1:27:23<20:28, 87.56batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|████      | 71680/179200 [1:27:24<20:19, 88.14batch/s, batch_nb=70399, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  40%|████      | 72047/179200 [1:27:54<2:12:12, 13.51batch/s, batch_nb=70766, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  42%|████▏     | 74827/179200 [1:31:42<2:09:37, 13.42batch/s, batch_nb=73546, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  43%|████▎     | 77235/179200 [1:35:00<2:23:26, 11.85batch/s, batch_nb=75954, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  44%|████▍     | 79467/179200 [1:37:55<2:00:05, 13.84batch/s, batch_nb=78186, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  46%|████▌     | 81575/179200 [1:40:42<2:06:58, 12.81batch/s, batch_nb=80294, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  47%|████▋     | 83741/179200 [1:43:32<1:46:48, 14.89batch/s, batch_nb=82460, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  48%|████▊     | 85795/179200 [1:46:18<2:14:15, 11.60batch/s, batch_nb=84514, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  49%|████▉     | 88127/179200 [1:49:32<1:56:40, 13.01batch/s, batch_nb=86846, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  50%|████▉     | 89280/179200 [1:51:07<1:42:54, 14.56batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 2:  50%|████▉     | 89289/179200 [1:51:07<1:17:20, 19.38batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89298/179200 [1:51:08<59:15, 25.29batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]  \n",
      "Epoch 2:  50%|████▉     | 89307/179200 [1:51:08<46:42, 32.07batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89316/179200 [1:51:08<37:48, 39.63batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89325/179200 [1:51:08<31:34, 47.43batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89334/179200 [1:51:08<27:19, 54.82batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89343/179200 [1:51:08<24:24, 61.36batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89353/179200 [1:51:08<22:03, 67.87batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89362/179200 [1:51:08<20:31, 72.95batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89372/179200 [1:51:08<19:18, 77.51batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89381/179200 [1:51:08<18:31, 80.78batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89390/179200 [1:51:09<18:00, 83.12batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89399/179200 [1:51:09<17:41, 84.59batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89408/179200 [1:51:09<17:34, 85.12batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89417/179200 [1:51:09<17:29, 85.55batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89426/179200 [1:51:09<17:14, 86.76batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89435/179200 [1:51:09<17:04, 87.58batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89445/179200 [1:51:09<16:52, 88.66batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89455/179200 [1:51:09<16:50, 88.85batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89465/179200 [1:51:09<16:46, 89.18batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89475/179200 [1:51:10<16:38, 89.82batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89485/179200 [1:51:10<16:39, 89.72batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89494/179200 [1:51:10<16:40, 89.64batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89504/179200 [1:51:10<16:41, 89.53batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89514/179200 [1:51:10<16:36, 89.98batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89524/179200 [1:51:10<16:46, 89.07batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89533/179200 [1:51:10<16:47, 88.98batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89543/179200 [1:51:10<16:44, 89.22batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89552/179200 [1:51:10<16:56, 88.22batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89561/179200 [1:51:10<16:55, 88.26batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89570/179200 [1:51:11<17:06, 87.28batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89579/179200 [1:51:11<16:59, 87.92batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|████▉     | 89588/179200 [1:51:11<17:03, 87.57batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  50%|█████     | 89600/179200 [1:51:12<17:12, 86.78batch/s, batch_nb=87999, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  51%|█████     | 90555/179200 [1:52:32<2:04:24, 11.88batch/s, batch_nb=88954, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  52%|█████▏    | 93351/179200 [1:56:25<1:36:11, 14.87batch/s, batch_nb=91750, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  52%|█████▏    | 93415/179200 [1:56:30<2:06:36, 11.29batch/s, batch_nb=91814, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  55%|█████▍    | 97695/179200 [2:02:28<1:55:01, 11.81batch/s, batch_nb=96094, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  56%|█████▌    | 99787/179200 [2:05:24<1:35:34, 13.85batch/s, batch_nb=98186, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  57%|█████▋    | 102535/179200 [2:09:18<1:48:45, 11.75batch/s, batch_nb=100934, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  57%|█████▋    | 102879/179200 [2:09:46<1:30:29, 14.06batch/s, batch_nb=101278, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  60%|█████▉    | 107067/179200 [2:15:41<1:35:21, 12.61batch/s, batch_nb=105466, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  60%|█████▉    | 107200/179200 [2:15:53<1:26:45, 13.83batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 2:  60%|█████▉    | 107209/179200 [2:15:53<1:04:56, 18.48batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107218/179200 [2:15:53<49:34, 24.20batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]  \n",
      "Epoch 2:  60%|█████▉    | 107227/179200 [2:15:53<38:44, 30.96batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107237/179200 [2:15:53<31:04, 38.59batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107246/179200 [2:15:53<25:51, 46.38batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107255/179200 [2:15:53<22:12, 53.99batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107264/179200 [2:15:53<19:46, 60.61batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107274/179200 [2:15:54<17:47, 67.39batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107283/179200 [2:15:54<16:35, 72.21batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107292/179200 [2:15:54<15:48, 75.81batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107301/179200 [2:15:54<15:04, 79.50batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107310/179200 [2:15:54<14:40, 81.68batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107320/179200 [2:15:54<14:16, 83.95batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107329/179200 [2:15:54<14:01, 85.43batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107339/179200 [2:15:54<13:44, 87.12batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107348/179200 [2:15:54<13:43, 87.24batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107357/179200 [2:15:54<13:48, 86.70batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107366/179200 [2:15:55<13:55, 86.02batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107375/179200 [2:15:55<13:57, 85.78batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107384/179200 [2:15:55<13:56, 85.83batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107393/179200 [2:15:55<13:52, 86.23batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107402/179200 [2:15:55<13:42, 87.29batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107411/179200 [2:15:55<13:39, 87.61batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107420/179200 [2:15:55<13:40, 87.53batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107429/179200 [2:15:55<13:38, 87.71batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107439/179200 [2:15:55<13:30, 88.51batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107448/179200 [2:15:56<13:29, 88.60batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107457/179200 [2:15:56<13:37, 87.71batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107466/179200 [2:15:56<13:36, 87.84batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107475/179200 [2:15:56<13:36, 87.82batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107484/179200 [2:15:56<13:31, 88.39batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107493/179200 [2:15:56<13:28, 88.69batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107502/179200 [2:15:56<13:34, 87.99batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|█████▉    | 107511/179200 [2:15:56<13:30, 88.48batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  60%|██████    | 107520/179200 [2:15:58<13:28, 88.60batch/s, batch_nb=105599, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  61%|██████    | 109551/179200 [2:18:54<1:21:06, 14.31batch/s, batch_nb=107630, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  62%|██████▏   | 111591/179200 [2:21:47<1:16:35, 14.71batch/s, batch_nb=109670, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  63%|██████▎   | 113671/179200 [2:24:43<1:15:07, 14.54batch/s, batch_nb=111750, gpu=0, loss=0.066, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  64%|██████▍   | 115251/179200 [2:26:58<1:14:29, 14.31batch/s, batch_nb=113330, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  67%|██████▋   | 119291/179200 [2:32:46<1:08:58, 14.48batch/s, batch_nb=117370, gpu=0, loss=0.065, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  68%|██████▊   | 121375/179200 [2:35:48<1:31:07, 10.58batch/s, batch_nb=119454, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  69%|██████▉   | 123635/179200 [2:39:03<1:27:39, 10.56batch/s, batch_nb=121714, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  70%|██████▉   | 125120/179200 [2:41:11<1:03:19, 14.23batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 2:  70%|██████▉   | 125129/179200 [2:41:11<47:35, 18.93batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]  \n",
      "Epoch 2:  70%|██████▉   | 125138/179200 [2:41:11<36:22, 24.78batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125147/179200 [2:41:11<28:27, 31.65batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125156/179200 [2:41:12<22:55, 39.30batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125166/179200 [2:41:12<18:58, 47.44batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125176/179200 [2:41:12<16:18, 55.24batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125185/179200 [2:41:12<14:25, 62.40batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125194/179200 [2:41:12<13:07, 68.61batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125203/179200 [2:41:12<12:18, 73.11batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125212/179200 [2:41:12<11:37, 77.39batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125222/179200 [2:41:12<11:08, 80.74batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125231/179200 [2:41:12<10:49, 83.10batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125240/179200 [2:41:12<10:42, 84.01batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125250/179200 [2:41:13<10:30, 85.55batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125259/179200 [2:41:13<10:29, 85.63batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125268/179200 [2:41:13<10:33, 85.15batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125277/179200 [2:41:13<10:30, 85.54batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125286/179200 [2:41:13<10:23, 86.47batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125295/179200 [2:41:13<10:23, 86.47batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125304/179200 [2:41:13<10:27, 85.94batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125313/179200 [2:41:13<10:19, 87.03batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125322/179200 [2:41:13<10:14, 87.66batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125331/179200 [2:41:14<10:12, 87.88batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125340/179200 [2:41:14<10:22, 86.49batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125349/179200 [2:41:14<10:16, 87.34batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125358/179200 [2:41:14<10:19, 86.86batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125367/179200 [2:41:14<10:15, 87.39batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125376/179200 [2:41:14<10:16, 87.34batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125385/179200 [2:41:14<10:11, 87.99batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125395/179200 [2:41:14<10:07, 88.52batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125404/179200 [2:41:14<10:07, 88.57batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125413/179200 [2:41:14<10:09, 88.22batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|██████▉   | 125422/179200 [2:41:15<10:07, 88.57batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|███████   | 125440/179200 [2:41:16<10:05, 88.84batch/s, batch_nb=123199, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 2:  70%|███████   | 126155/179200 [2:42:19<1:19:00, 11.19batch/s, batch_nb=123914, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  72%|███████▏  | 128640/179200 [2:45:54<56:55, 14.80batch/s, batch_nb=126399, gpu=0, loss=0.063, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  73%|███████▎  | 130667/179200 [2:48:49<1:03:22, 12.76batch/s, batch_nb=128426, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  74%|███████▍  | 132679/179200 [2:51:46<58:47, 13.19batch/s, batch_nb=130438, gpu=0, loss=0.065, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  75%|███████▌  | 134721/179200 [2:54:45<52:45, 14.05batch/s, batch_nb=132480, gpu=0, loss=0.065, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  76%|███████▌  | 136155/179200 [2:56:53<1:06:18, 10.82batch/s, batch_nb=133914, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  78%|███████▊  | 140195/179200 [3:02:39<54:47, 11.87batch/s, batch_nb=137954, gpu=0, loss=0.065, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  79%|███████▉  | 142261/179200 [3:05:29<38:26, 16.02batch/s, batch_nb=140020, gpu=0, loss=0.064, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|███████▉  | 142927/179200 [3:06:20<48:37, 12.43batch/s, batch_nb=140686, gpu=0, loss=0.064, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|███████▉  | 143040/179200 [3:06:28<40:35, 14.85batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]  \n",
      "Epoch 2:  80%|███████▉  | 143044/179200 [3:06:29<32:21, 18.63batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143055/179200 [3:06:29<24:23, 24.69batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143065/179200 [3:06:29<18:52, 31.89batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143076/179200 [3:06:29<14:59, 40.15batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143087/179200 [3:06:29<12:15, 49.11batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143098/179200 [3:06:29<10:19, 58.23batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143109/179200 [3:06:29<08:59, 66.92batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143120/179200 [3:06:29<08:02, 74.72batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143131/179200 [3:06:29<07:22, 81.45batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143142/179200 [3:06:29<06:55, 86.85batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143153/179200 [3:06:30<06:35, 91.09batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143164/179200 [3:06:30<06:26, 93.17batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143175/179200 [3:06:30<06:16, 95.80batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143186/179200 [3:06:30<06:08, 97.76batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143197/179200 [3:06:30<06:02, 99.27batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143208/179200 [3:06:30<05:58, 100.41batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143219/179200 [3:06:30<05:55, 101.17batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143230/179200 [3:06:30<05:53, 101.84batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143241/179200 [3:06:30<05:51, 102.19batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143252/179200 [3:06:31<05:51, 102.33batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143263/179200 [3:06:31<05:50, 102.48batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143274/179200 [3:06:31<05:49, 102.67batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143285/179200 [3:06:31<05:49, 102.88batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143296/179200 [3:06:31<05:48, 103.03batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143307/179200 [3:06:31<05:48, 103.09batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143318/179200 [3:06:31<05:47, 103.18batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143329/179200 [3:06:31<05:47, 103.32batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143340/179200 [3:06:31<05:47, 103.33batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|███████▉  | 143351/179200 [3:06:31<05:46, 103.38batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  80%|████████  | 143360/179200 [3:06:33<05:46, 103.38batch/s, batch_nb=140799, gpu=0, loss=0.066, v_nb=55]\n",
      "Epoch 2:  82%|████████▏ | 147639/179200 [3:12:02<38:22, 13.71batch/s, batch_nb=145078, gpu=0, loss=0.061, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  84%|████████▎ | 149655/179200 [3:14:37<36:31, 13.48batch/s, batch_nb=147094, gpu=0, loss=0.065, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  85%|████████▍ | 152311/179200 [3:18:02<28:51, 15.53batch/s, batch_nb=149750, gpu=0, loss=0.064, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  86%|████████▌ | 154339/179200 [3:20:39<28:14, 14.67batch/s, batch_nb=151778, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  87%|████████▋ | 156441/179200 [3:23:22<24:04, 15.76batch/s, batch_nb=153880, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  89%|████████▊ | 158779/179200 [3:26:24<25:58, 13.10batch/s, batch_nb=156218, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  90%|████████▉ | 160855/179200 [3:29:06<23:18, 13.11batch/s, batch_nb=158294, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  90%|████████▉ | 160960/179200 [3:29:14<22:31, 13.49batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 160964/179200 [3:29:14<17:42, 17.16batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 160975/179200 [3:29:14<13:16, 22.89batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 160986/179200 [3:29:14<10:10, 29.86batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 160997/179200 [3:29:14<07:59, 37.96batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161008/179200 [3:29:14<06:27, 46.90batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161019/179200 [3:29:14<05:24, 56.09batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161030/179200 [3:29:14<04:39, 65.05batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161041/179200 [3:29:15<04:07, 73.26batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161052/179200 [3:29:15<03:45, 80.37batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161062/179200 [3:29:15<03:33, 85.00batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161072/179200 [3:29:15<03:26, 87.93batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161082/179200 [3:29:15<03:20, 90.46batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161092/179200 [3:29:15<03:20, 90.42batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161102/179200 [3:29:15<03:19, 90.83batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161113/179200 [3:29:15<03:11, 94.32batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161124/179200 [3:29:15<03:06, 97.04batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161135/179200 [3:29:16<03:02, 98.99batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161146/179200 [3:29:16<02:59, 100.40batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161157/179200 [3:29:16<02:57, 101.56batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161168/179200 [3:29:16<02:56, 102.28batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161179/179200 [3:29:16<02:55, 102.71batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161190/179200 [3:29:16<02:55, 102.73batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161201/179200 [3:29:16<02:55, 102.61batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161212/179200 [3:29:16<02:54, 103.07batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161223/179200 [3:29:16<02:53, 103.44batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161234/179200 [3:29:16<02:52, 103.86batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161245/179200 [3:29:17<02:52, 104.19batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161256/179200 [3:29:17<02:51, 104.37batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|████████▉ | 161267/179200 [3:29:17<02:51, 104.52batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  90%|█████████ | 161280/179200 [3:29:18<02:51, 104.73batch/s, batch_nb=158399, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 2:  91%|█████████▏| 163580/179200 [3:32:20<16:08, 16.12batch/s, batch_nb=160699, gpu=0, loss=0.064, v_nb=55] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  93%|█████████▎| 165767/179200 [3:35:11<16:21, 13.68batch/s, batch_nb=162886, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  94%|█████████▍| 168059/179200 [3:38:12<12:38, 14.68batch/s, batch_nb=165178, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  95%|█████████▍| 170075/179200 [3:40:51<12:31, 12.15batch/s, batch_nb=167194, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  96%|█████████▌| 172415/179200 [3:43:55<08:33, 13.22batch/s, batch_nb=169534, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  97%|█████████▋| 174560/179200 [3:46:44<05:36, 13.77batch/s, batch_nb=171679, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  99%|█████████▊| 176571/179200 [3:49:23<02:49, 15.55batch/s, batch_nb=173690, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████▉| 178621/179200 [3:52:06<00:39, 14.73batch/s, batch_nb=175740, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████▉| 178880/179200 [3:52:26<00:21, 14.58batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 178884/179200 [3:52:26<00:17, 18.42batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 178895/179200 [3:52:26<00:12, 24.43batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 178906/179200 [3:52:27<00:09, 31.67batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 178917/179200 [3:52:27<00:07, 39.96batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 178928/179200 [3:52:27<00:05, 48.90batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 178939/179200 [3:52:27<00:04, 58.03batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 178949/179200 [3:52:27<00:03, 66.11batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 178960/179200 [3:52:27<00:03, 74.13batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 178971/179200 [3:52:27<00:02, 81.06batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 178982/179200 [3:52:27<00:02, 86.74batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 178993/179200 [3:52:27<00:02, 91.14batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179004/179200 [3:52:27<00:02, 94.57batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179015/179200 [3:52:28<00:01, 97.19batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179026/179200 [3:52:28<00:01, 99.06batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179037/179200 [3:52:28<00:01, 100.43batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179048/179200 [3:52:28<00:01, 101.37batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179059/179200 [3:52:28<00:01, 102.16batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179070/179200 [3:52:28<00:01, 102.44batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179081/179200 [3:52:28<00:01, 102.65batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179092/179200 [3:52:28<00:01, 102.49batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179103/179200 [3:52:28<00:00, 102.77batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179114/179200 [3:52:29<00:00, 102.86batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179125/179200 [3:52:29<00:00, 103.05batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179136/179200 [3:52:29<00:00, 103.18batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179147/179200 [3:52:29<00:00, 103.23batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179158/179200 [3:52:29<00:00, 103.30batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179169/179200 [3:52:29<00:00, 103.35batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179180/179200 [3:52:29<00:00, 103.28batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|█████████▉| 179191/179200 [3:52:29<00:00, 103.29batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 2: 100%|██████████| 179200/179200 [3:52:31<00:00, 103.29batch/s, batch_nb=175999, gpu=0, loss=0.065, v_nb=55]\n",
      "Epoch 3:   2%|▏         | 2699/179200 [03:33<3:17:41, 14.88batch/s, batch_nb=2698, gpu=0, loss=0.063, v_nb=55]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   3%|▎         | 4915/179200 [06:30<4:13:37, 11.45batch/s, batch_nb=4914, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   4%|▍         | 6915/179200 [09:08<3:58:19, 12.05batch/s, batch_nb=6914, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   5%|▍         | 8947/179200 [11:50<3:30:48, 13.46batch/s, batch_nb=8946, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   5%|▌         | 9591/179200 [12:41<3:13:43, 14.59batch/s, batch_nb=9590, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   8%|▊         | 14131/179200 [18:46<3:07:11, 14.70batch/s, batch_nb=14130, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   9%|▊         | 15427/179200 [20:29<3:29:38, 13.02batch/s, batch_nb=15426, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  10%|▉         | 17600/179200 [23:23<2:53:22, 15.53batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17604/179200 [23:23<2:18:17, 19.47batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17615/179200 [23:24<1:44:39, 25.73batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17626/179200 [23:24<1:21:07, 33.19batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17637/179200 [23:24<1:04:37, 41.67batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17648/179200 [23:24<53:04, 50.74batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]  \n",
      "Epoch 3:  10%|▉         | 17659/179200 [23:24<44:59, 59.85batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17670/179200 [23:24<39:20, 68.44batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17681/179200 [23:24<35:23, 76.08batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17692/179200 [23:24<32:44, 82.20batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17703/179200 [23:24<30:41, 87.72batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17714/179200 [23:25<29:13, 92.07batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17725/179200 [23:25<28:13, 95.35batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17736/179200 [23:25<27:31, 97.75batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17747/179200 [23:25<27:03, 99.47batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17758/179200 [23:25<26:41, 100.78batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17769/179200 [23:25<26:28, 101.62batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17780/179200 [23:25<26:17, 102.29batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17791/179200 [23:25<26:12, 102.63batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17802/179200 [23:25<26:07, 102.94batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17813/179200 [23:25<26:04, 103.18batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17824/179200 [23:26<26:00, 103.42batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17835/179200 [23:26<25:58, 103.54batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17846/179200 [23:26<25:56, 103.69batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17857/179200 [23:26<25:54, 103.82batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17868/179200 [23:26<25:54, 103.81batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17879/179200 [23:26<25:55, 103.74batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17890/179200 [23:26<25:59, 103.46batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17901/179200 [23:26<25:56, 103.60batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|▉         | 17912/179200 [23:26<25:56, 103.64batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  10%|█         | 17920/179200 [23:28<25:56, 103.64batch/s, batch_nb=17599, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  11%|█         | 19747/179200 [25:55<3:52:19, 11.44batch/s, batch_nb=19426, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  12%|█▏        | 21995/179200 [28:57<3:43:44, 11.71batch/s, batch_nb=21674, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  13%|█▎        | 23680/179200 [31:13<3:07:18, 13.84batch/s, batch_nb=23359, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  15%|█▌        | 27695/179200 [36:38<3:17:58, 12.75batch/s, batch_nb=27374, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  16%|█▌        | 28511/179200 [37:44<3:01:48, 13.81batch/s, batch_nb=28190, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  18%|█▊        | 32575/179200 [43:19<3:29:59, 11.64batch/s, batch_nb=32254, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  19%|█▊        | 33171/179200 [44:06<2:48:08, 14.48batch/s, batch_nb=32850, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  20%|█▉        | 35520/179200 [47:28<2:51:35, 13.96batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35530/179200 [47:28<2:07:21, 18.80batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35541/179200 [47:28<1:36:10, 24.90batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35552/179200 [47:28<1:14:18, 32.22batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35563/179200 [47:28<58:57, 40.60batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]  \n",
      "Epoch 3:  20%|█▉        | 35574/179200 [47:28<48:23, 49.47batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35585/179200 [47:28<40:56, 58.47batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35595/179200 [47:28<36:18, 65.91batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35606/179200 [47:29<32:26, 73.76batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35616/179200 [47:29<30:08, 79.42batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35627/179200 [47:29<28:06, 85.14batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35638/179200 [47:29<26:42, 89.60batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35649/179200 [47:29<25:48, 92.72batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35660/179200 [47:29<25:08, 95.15batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35671/179200 [47:29<24:40, 96.96batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35682/179200 [47:29<24:21, 98.22batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35693/179200 [47:29<24:17, 98.44batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35704/179200 [47:30<24:27, 97.78batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35714/179200 [47:30<24:47, 96.46batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35725/179200 [47:30<24:16, 98.51batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35735/179200 [47:30<24:22, 98.10batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35745/179200 [47:30<24:21, 98.18batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35756/179200 [47:30<23:56, 99.86batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35767/179200 [47:30<23:44, 100.70batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35778/179200 [47:30<23:35, 101.33batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35789/179200 [47:30<23:29, 101.76batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35800/179200 [47:30<23:25, 102.06batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35811/179200 [47:31<23:23, 102.13batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35822/179200 [47:31<23:19, 102.45batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|█▉        | 35833/179200 [47:31<23:18, 102.51batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  20%|██        | 35840/179200 [47:32<23:18, 102.51batch/s, batch_nb=35199, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  21%|██        | 37461/179200 [49:51<2:29:36, 15.79batch/s, batch_nb=36820, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  23%|██▎       | 41535/179200 [55:31<3:27:02, 11.08batch/s, batch_nb=40894, gpu=0, loss=0.064, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  24%|██▍       | 43567/179200 [58:24<3:15:00, 11.59batch/s, batch_nb=42926, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  26%|██▌       | 45751/179200 [1:01:26<3:03:39, 12.11batch/s, batch_nb=45110, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  27%|██▋       | 47831/179200 [1:04:19<2:33:04, 14.30batch/s, batch_nb=47190, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  28%|██▊       | 49755/179200 [1:07:02<3:52:22,  9.28batch/s, batch_nb=49114, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  30%|██▉       | 53440/179200 [1:12:11<2:18:45, 15.10batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53449/179200 [1:12:11<1:44:15, 20.10batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53458/179200 [1:12:11<1:19:59, 26.20batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53468/179200 [1:12:11<1:02:51, 33.33batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53479/179200 [1:12:11<50:14, 41.70batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]  \n",
      "Epoch 3:  30%|██▉       | 53489/179200 [1:12:11<41:42, 50.24batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53499/179200 [1:12:11<36:10, 57.93batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53509/179200 [1:12:11<32:01, 65.40batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53520/179200 [1:12:11<28:29, 73.52batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53531/179200 [1:12:12<26:01, 80.46batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53541/179200 [1:12:12<24:46, 84.55batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53551/179200 [1:12:12<23:39, 88.51batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53561/179200 [1:12:12<22:52, 91.57batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53572/179200 [1:12:12<22:03, 94.93batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53583/179200 [1:12:12<21:37, 96.79batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53594/179200 [1:12:12<21:30, 97.30batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53605/179200 [1:12:12<21:14, 98.53batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53616/179200 [1:12:12<21:19, 98.12batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53627/179200 [1:12:13<21:01, 99.53batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53638/179200 [1:12:13<20:46, 100.74batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53649/179200 [1:12:13<20:36, 101.57batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53660/179200 [1:12:13<20:28, 102.20batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53671/179200 [1:12:13<20:24, 102.54batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53682/179200 [1:12:13<20:40, 101.16batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53693/179200 [1:12:13<20:31, 101.89batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53704/179200 [1:12:13<20:38, 101.29batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53715/179200 [1:12:13<20:35, 101.54batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53726/179200 [1:12:13<20:28, 102.10batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53737/179200 [1:12:14<20:23, 102.55batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|██▉       | 53748/179200 [1:12:14<20:37, 101.39batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  30%|███       | 53760/179200 [1:12:15<21:05, 99.12batch/s, batch_nb=52799, gpu=0, loss=0.062, v_nb=55] \n",
      "Epoch 3:  31%|███       | 54751/179200 [1:13:39<2:24:13, 14.38batch/s, batch_nb=53790, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  32%|███▏      | 57241/179200 [1:17:09<2:10:23, 15.59batch/s, batch_nb=56280, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 59387/179200 [1:20:09<3:01:33, 11.00batch/s, batch_nb=58426, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 59901/179200 [1:20:52<2:10:58, 15.18batch/s, batch_nb=58940, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  35%|███▌      | 63519/179200 [1:25:59<2:27:11, 13.10batch/s, batch_nb=62558, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  37%|███▋      | 66811/179200 [1:30:38<2:12:07, 14.18batch/s, batch_nb=65850, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  40%|███▉      | 70907/179200 [1:36:31<2:24:30, 12.49batch/s, batch_nb=69946, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  40%|███▉      | 71360/179200 [1:37:10<2:01:40, 14.77batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 3:  40%|███▉      | 71370/179200 [1:37:10<1:30:46, 19.80batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71381/179200 [1:37:11<1:08:47, 26.12batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71392/179200 [1:37:11<53:22, 33.66batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]  \n",
      "Epoch 3:  40%|███▉      | 71402/179200 [1:37:11<42:51, 41.93batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71412/179200 [1:37:11<35:38, 50.42batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71422/179200 [1:37:11<30:39, 58.58batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71432/179200 [1:37:11<27:02, 66.42batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71443/179200 [1:37:11<24:11, 74.23batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71454/179200 [1:37:11<22:14, 80.73batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71465/179200 [1:37:11<20:54, 85.85batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71476/179200 [1:37:12<19:58, 89.86batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71487/179200 [1:37:12<19:18, 92.94batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71497/179200 [1:37:12<18:57, 94.73batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71508/179200 [1:37:12<18:35, 96.57batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71519/179200 [1:37:12<18:16, 98.17batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71530/179200 [1:37:12<18:07, 99.01batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71541/179200 [1:37:12<18:02, 99.47batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71552/179200 [1:37:12<17:58, 99.81batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71563/179200 [1:37:12<17:54, 100.18batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71574/179200 [1:37:13<17:49, 100.62batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71585/179200 [1:37:13<17:46, 100.86batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71596/179200 [1:37:13<17:46, 100.89batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71607/179200 [1:37:13<17:43, 101.19batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71618/179200 [1:37:13<17:38, 101.64batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71629/179200 [1:37:13<17:37, 101.77batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71640/179200 [1:37:13<17:35, 101.93batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71651/179200 [1:37:13<17:34, 101.94batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|███▉      | 71662/179200 [1:37:13<17:35, 101.84batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|████      | 71680/179200 [1:37:15<17:34, 101.93batch/s, batch_nb=70399, gpu=0, loss=0.064, v_nb=55]\n",
      "Epoch 3:  40%|████      | 72431/179200 [1:38:24<2:02:57, 14.47batch/s, batch_nb=71150, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  43%|████▎     | 77141/179200 [1:45:24<1:50:58, 15.33batch/s, batch_nb=75860, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  43%|████▎     | 77901/179200 [1:46:30<1:54:32, 14.74batch/s, batch_nb=76620, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  45%|████▌     | 81295/179200 [1:51:27<2:36:19, 10.44batch/s, batch_nb=80014, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  48%|████▊     | 85407/179200 [1:57:29<2:05:27, 12.46batch/s, batch_nb=84126, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  49%|████▉     | 87661/179200 [2:00:49<1:41:28, 15.03batch/s, batch_nb=86380, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  50%|████▉     | 89280/179200 [2:03:14<1:51:06, 13.49batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89283/179200 [2:03:14<1:29:22, 16.77batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89293/179200 [2:03:14<1:07:08, 22.32batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89303/179200 [2:03:14<51:31, 29.08batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]  \n",
      "Epoch 3:  50%|████▉     | 89313/179200 [2:03:14<40:41, 36.82batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89324/179200 [2:03:14<32:56, 45.48batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89334/179200 [2:03:14<27:37, 54.21batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89345/179200 [2:03:14<23:45, 63.04batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89356/179200 [2:03:15<21:06, 70.92batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89367/179200 [2:03:15<19:15, 77.72batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89377/179200 [2:03:15<18:07, 82.57batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89387/179200 [2:03:15<17:12, 87.01batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89398/179200 [2:03:15<16:28, 90.81batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89408/179200 [2:03:15<16:04, 93.07batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89418/179200 [2:03:15<16:05, 93.02batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89428/179200 [2:03:15<15:49, 94.56batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89438/179200 [2:03:15<15:34, 96.01batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89448/179200 [2:03:15<15:25, 97.01batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89458/179200 [2:03:16<15:18, 97.75batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89468/179200 [2:03:16<15:12, 98.33batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89478/179200 [2:03:16<15:09, 98.70batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89488/179200 [2:03:16<15:09, 98.60batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89498/179200 [2:03:16<15:07, 98.82batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89508/179200 [2:03:16<15:08, 98.74batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89518/179200 [2:03:16<15:07, 98.85batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89528/179200 [2:03:16<15:04, 99.15batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89538/179200 [2:03:16<15:07, 98.83batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89548/179200 [2:03:17<15:04, 99.09batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89558/179200 [2:03:17<15:04, 99.13batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89569/179200 [2:03:17<15:01, 99.44batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89579/179200 [2:03:17<15:02, 99.31batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|████▉     | 89589/179200 [2:03:17<15:07, 98.75batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|█████     | 89600/179200 [2:03:19<15:04, 99.11batch/s, batch_nb=87999, gpu=0, loss=0.063, v_nb=55]\n",
      "Epoch 3:  50%|█████     | 90181/179200 [2:04:10<1:39:24, 14.93batch/s, batch_nb=88580, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  52%|█████▏    | 93139/179200 [2:08:33<1:42:56, 13.93batch/s, batch_nb=91538, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  53%|█████▎    | 95355/179200 [2:11:50<1:57:31, 11.89batch/s, batch_nb=93754, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  54%|█████▍    | 97281/179200 [2:14:40<1:33:27, 14.61batch/s, batch_nb=95680, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  57%|█████▋    | 101447/179200 [2:20:57<1:55:32, 11.22batch/s, batch_nb=99846, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  58%|█████▊    | 103561/179200 [2:24:19<1:28:21, 14.27batch/s, batch_nb=101960, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  59%|█████▉    | 105661/179200 [2:27:38<1:25:43, 14.30batch/s, batch_nb=104060, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  60%|█████▉    | 107200/179200 [2:30:02<1:26:52, 13.81batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 3:  60%|█████▉    | 107210/179200 [2:30:02<1:04:50, 18.50batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107220/179200 [2:30:03<49:10, 24.39batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]  \n",
      "Epoch 3:  60%|█████▉    | 107230/179200 [2:30:03<38:07, 31.46batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107240/179200 [2:30:03<30:25, 39.41batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107250/179200 [2:30:03<25:06, 47.77batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107260/179200 [2:30:03<21:24, 56.00batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107270/179200 [2:30:03<18:46, 63.87batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107280/179200 [2:30:03<16:51, 71.13batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107290/179200 [2:30:03<15:35, 76.89batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107300/179200 [2:30:03<14:38, 81.85batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107310/179200 [2:30:04<14:00, 85.49batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107320/179200 [2:30:04<13:32, 88.50batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107330/179200 [2:30:04<13:14, 90.41batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107340/179200 [2:30:04<13:00, 92.04batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107350/179200 [2:30:04<12:51, 93.18batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107360/179200 [2:30:04<12:42, 94.26batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107370/179200 [2:30:04<12:38, 94.74batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107380/179200 [2:30:04<12:39, 94.54batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107390/179200 [2:30:04<12:40, 94.47batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107400/179200 [2:30:04<12:36, 94.91batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107410/179200 [2:30:05<12:36, 94.84batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107420/179200 [2:30:05<12:34, 95.08batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107430/179200 [2:30:05<12:33, 95.21batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107440/179200 [2:30:05<12:40, 94.30batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107450/179200 [2:30:05<12:43, 93.92batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107460/179200 [2:30:05<12:44, 93.79batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107470/179200 [2:30:05<12:38, 94.61batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107480/179200 [2:30:05<12:36, 94.84batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107490/179200 [2:30:05<12:31, 95.41batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107500/179200 [2:30:06<12:33, 95.09batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|█████▉    | 107510/179200 [2:30:06<12:30, 95.51batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|██████    | 107520/179200 [2:30:07<12:32, 95.30batch/s, batch_nb=105599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  60%|██████    | 108141/179200 [2:31:06<1:26:02, 13.76batch/s, batch_nb=106220, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  61%|██████    | 109407/179200 [2:33:04<1:39:01, 11.75batch/s, batch_nb=107486, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  63%|██████▎   | 112661/179200 [2:38:09<1:18:04, 14.20batch/s, batch_nb=110740, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  65%|██████▌   | 117271/179200 [2:45:24<1:12:26, 14.25batch/s, batch_nb=115350, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  66%|██████▌   | 118151/179200 [2:46:48<1:11:44, 14.18batch/s, batch_nb=116230, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  68%|██████▊   | 120981/179200 [2:51:15<1:15:35, 12.84batch/s, batch_nb=119060, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  70%|██████▉   | 125120/179200 [2:57:58<1:07:03, 13.44batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125129/179200 [2:57:58<49:56, 18.04batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]  \n",
      "Epoch 3:  70%|██████▉   | 125139/179200 [2:57:58<37:45, 23.86batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125149/179200 [2:57:58<29:17, 30.76batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125159/179200 [2:57:58<23:18, 38.64batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125168/179200 [2:57:59<19:19, 46.62batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125178/179200 [2:57:59<16:27, 54.72batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125188/179200 [2:57:59<14:24, 62.49batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125198/179200 [2:57:59<12:56, 69.52batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125208/179200 [2:57:59<11:55, 75.46batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125218/179200 [2:57:59<11:14, 80.09batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125228/179200 [2:57:59<10:47, 83.33batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125238/179200 [2:57:59<10:30, 85.59batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125248/179200 [2:57:59<10:17, 87.41batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125258/179200 [2:58:00<10:03, 89.40batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125268/179200 [2:58:00<09:54, 90.77batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125278/179200 [2:58:00<09:56, 90.42batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125288/179200 [2:58:00<09:52, 91.06batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125298/179200 [2:58:00<09:54, 90.69batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125308/179200 [2:58:00<09:50, 91.29batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125318/179200 [2:58:00<09:46, 91.95batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125328/179200 [2:58:00<09:46, 91.84batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125338/179200 [2:58:00<09:44, 92.10batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125348/179200 [2:58:00<09:43, 92.36batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125358/179200 [2:58:01<09:53, 90.70batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125368/179200 [2:58:01<09:58, 90.01batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125378/179200 [2:58:01<09:57, 90.09batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125388/179200 [2:58:01<09:55, 90.40batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125398/179200 [2:58:01<09:54, 90.44batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125408/179200 [2:58:01<09:50, 91.02batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125418/179200 [2:58:01<09:54, 90.52batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125428/179200 [2:58:01<09:53, 90.55batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|██████▉   | 125438/179200 [2:58:01<09:50, 91.01batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|███████   | 125440/179200 [2:58:03<09:50, 91.01batch/s, batch_nb=123199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  70%|███████   | 125621/179200 [2:58:21<1:05:19, 13.67batch/s, batch_nb=123380, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  71%|███████   | 126759/179200 [3:00:12<1:12:15, 12.09batch/s, batch_nb=124518, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  74%|███████▎  | 131739/179200 [3:08:13<1:00:53, 12.99batch/s, batch_nb=129498, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  75%|███████▍  | 133935/179200 [3:11:46<1:15:28, 10.00batch/s, batch_nb=131694, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  76%|███████▌  | 135959/179200 [3:15:02<58:31, 12.31batch/s, batch_nb=133718, gpu=0, loss=0.063, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  77%|███████▋  | 138171/179200 [3:18:39<50:12, 13.62batch/s, batch_nb=135930, gpu=0, loss=0.062, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  79%|███████▊  | 140739/179200 [3:22:48<50:00, 12.82batch/s, batch_nb=138498, gpu=0, loss=0.061, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|███████▉  | 143040/179200 [3:26:32<44:45, 13.47batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]  \n",
      "Epoch 3:  80%|███████▉  | 143049/179200 [3:26:33<33:19, 18.08batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143059/179200 [3:26:33<25:13, 23.88batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143069/179200 [3:26:33<19:29, 30.88batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143079/179200 [3:26:33<15:30, 38.83batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143089/179200 [3:26:33<12:43, 47.30batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143098/179200 [3:26:33<10:55, 55.10batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143108/179200 [3:26:33<09:30, 63.24batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143117/179200 [3:26:33<08:52, 67.75batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143127/179200 [3:26:33<08:09, 73.63batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143136/179200 [3:26:33<07:56, 75.73batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143145/179200 [3:26:34<07:46, 77.34batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143154/179200 [3:26:34<07:38, 78.63batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143163/179200 [3:26:34<07:23, 81.20batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143173/179200 [3:26:34<07:02, 85.27batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143183/179200 [3:26:34<06:48, 88.19batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143193/179200 [3:26:34<06:38, 90.29batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143203/179200 [3:26:34<06:32, 91.79batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143213/179200 [3:26:34<06:27, 92.97batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143223/179200 [3:26:34<06:23, 93.83batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143233/179200 [3:26:35<06:20, 94.53batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143243/179200 [3:26:35<06:16, 95.51batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143253/179200 [3:26:35<06:15, 95.63batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143263/179200 [3:26:35<06:13, 96.15batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143273/179200 [3:26:35<06:16, 95.48batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143283/179200 [3:26:35<06:22, 94.00batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143293/179200 [3:26:35<06:20, 94.40batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143303/179200 [3:26:35<06:23, 93.53batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143313/179200 [3:26:35<06:22, 93.89batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143323/179200 [3:26:35<06:19, 94.66batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143333/179200 [3:26:36<06:16, 95.31batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143343/179200 [3:26:36<06:19, 94.55batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|███████▉  | 143353/179200 [3:26:36<06:19, 94.50batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|████████  | 143360/179200 [3:26:38<06:19, 94.50batch/s, batch_nb=140799, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3:  80%|████████  | 143695/179200 [3:27:13<58:55, 10.04batch/s, batch_nb=141134, gpu=0, loss=0.062, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  81%|████████▏ | 145699/179200 [3:30:27<42:25, 13.16batch/s, batch_nb=143138, gpu=0, loss=0.061, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  83%|████████▎ | 148071/179200 [3:34:16<42:28, 12.21batch/s, batch_nb=145510, gpu=0, loss=0.062, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  84%|████████▍ | 150551/179200 [3:38:17<34:06, 14.00batch/s, batch_nb=147990, gpu=0, loss=0.061, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  85%|████████▌ | 152939/179200 [3:42:10<32:59, 13.26batch/s, batch_nb=150378, gpu=0, loss=0.063, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  87%|████████▋ | 155141/179200 [3:45:44<29:20, 13.67batch/s, batch_nb=152580, gpu=0, loss=0.062, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  88%|████████▊ | 157741/179200 [3:49:59<26:15, 13.62batch/s, batch_nb=155180, gpu=0, loss=0.064, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  90%|████████▉ | 160851/179200 [3:55:04<22:19, 13.70batch/s, batch_nb=158290, gpu=0, loss=0.061, v_nb=55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  90%|████████▉ | 160960/179200 [3:55:14<22:44, 13.37batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 3:  90%|████████▉ | 160970/179200 [3:55:14<16:54, 17.97batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 160980/179200 [3:55:15<12:46, 23.78batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 160990/179200 [3:55:15<09:52, 30.74batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161000/179200 [3:55:15<07:51, 38.60batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161010/179200 [3:55:15<06:27, 46.93batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161020/179200 [3:55:15<05:28, 55.38batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161030/179200 [3:55:15<04:46, 63.36batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161040/179200 [3:55:15<04:16, 70.78batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161050/179200 [3:55:15<03:56, 76.73batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161060/179200 [3:55:15<03:40, 82.11batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161070/179200 [3:55:16<03:30, 86.32batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161080/179200 [3:55:16<03:22, 89.26batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161090/179200 [3:55:16<03:18, 91.45batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161100/179200 [3:55:16<03:16, 92.21batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161110/179200 [3:55:16<03:16, 92.23batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161120/179200 [3:55:16<03:14, 93.00batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161130/179200 [3:55:16<03:13, 93.48batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161140/179200 [3:55:16<03:12, 93.62batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161150/179200 [3:55:16<03:13, 93.33batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161160/179200 [3:55:16<03:12, 93.74batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161170/179200 [3:55:17<03:10, 94.59batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161180/179200 [3:55:17<03:10, 94.64batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161190/179200 [3:55:17<03:11, 94.20batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161200/179200 [3:55:17<03:11, 94.09batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161210/179200 [3:55:17<03:09, 94.86batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161220/179200 [3:55:17<03:08, 95.40batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161230/179200 [3:55:17<03:07, 95.83batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161240/179200 [3:55:17<03:10, 94.48batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161250/179200 [3:55:17<03:09, 94.63batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161260/179200 [3:55:18<03:08, 94.98batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|████████▉ | 161270/179200 [3:55:18<03:08, 95.03batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  90%|█████████ | 161280/179200 [3:55:19<03:08, 95.29batch/s, batch_nb=158399, gpu=0, loss=0.062, v_nb=55]\n",
      "Epoch 3:  91%|█████████▏| 163719/179200 [3:59:19<20:21, 12.68batch/s, batch_nb=160838, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  93%|█████████▎| 166471/179200 [4:03:47<15:19, 13.85batch/s, batch_nb=163590, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  94%|█████████▍| 168719/179200 [4:07:26<13:29, 12.94batch/s, batch_nb=165838, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  96%|█████████▌| 171255/179200 [4:11:36<13:57,  9.48batch/s, batch_nb=168374, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  97%|█████████▋| 174255/179200 [4:16:29<07:08, 11.55batch/s, batch_nb=171374, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  98%|█████████▊| 176336/179200 [4:19:44<04:13, 11.31batch/s, batch_nb=173455, gpu=0, loss=0.063, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████▉| 178431/179200 [4:23:01<00:58, 13.07batch/s, batch_nb=175550, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████▉| 178880/179200 [4:23:45<00:23, 13.43batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Validating:   0%|          | 0/320 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 178890/179200 [4:23:45<00:17, 18.10batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 178901/179200 [4:23:45<00:12, 24.03batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 178911/179200 [4:23:45<00:09, 30.94batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 178922/179200 [4:23:45<00:07, 39.07batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 178933/179200 [4:23:45<00:05, 48.03batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 178944/179200 [4:23:45<00:04, 57.16batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 178955/179200 [4:23:45<00:03, 66.00batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 178966/179200 [4:23:46<00:03, 74.00batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 178976/179200 [4:23:46<00:02, 78.72batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 178986/179200 [4:23:46<00:02, 83.12batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 178996/179200 [4:23:46<00:02, 86.18batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179006/179200 [4:23:46<00:02, 88.89batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179016/179200 [4:23:46<00:02, 90.63batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179026/179200 [4:23:46<00:01, 91.76batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179036/179200 [4:23:46<00:01, 92.46batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179046/179200 [4:23:46<00:01, 92.84batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179056/179200 [4:23:47<00:01, 93.05batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179066/179200 [4:23:47<00:01, 92.83batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179076/179200 [4:23:47<00:01, 92.50batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179086/179200 [4:23:47<00:01, 92.64batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179096/179200 [4:23:47<00:01, 92.82batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179106/179200 [4:23:47<00:01, 91.21batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179116/179200 [4:23:47<00:00, 91.20batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179126/179200 [4:23:47<00:00, 91.18batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179136/179200 [4:23:47<00:00, 91.34batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179146/179200 [4:23:48<00:00, 91.46batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179156/179200 [4:23:48<00:00, 91.72batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179166/179200 [4:23:48<00:00, 91.15batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179176/179200 [4:23:48<00:00, 91.89batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|█████████▉| 179186/179200 [4:23:48<00:00, 91.86batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 3: 100%|██████████| 179200/179200 [4:23:50<00:00, 92.05batch/s, batch_nb=175999, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:   1%|          | 1855/179200 [02:57<4:26:15, 11.10batch/s, batch_nb=1854, gpu=0, loss=0.060, v_nb=55]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   3%|▎         | 4579/179200 [07:21<3:42:03, 13.11batch/s, batch_nb=4578, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   4%|▎         | 6675/179200 [10:42<4:55:07,  9.74batch/s, batch_nb=6674, gpu=0, loss=0.062, v_nb=55] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   5%|▍         | 8887/179200 [14:14<4:04:51, 11.59batch/s, batch_nb=8886, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   6%|▋         | 11641/179200 [18:36<3:24:57, 13.63batch/s, batch_nb=11640, gpu=0, loss=0.060, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   8%|▊         | 13991/179200 [22:18<3:09:33, 14.53batch/s, batch_nb=13990, gpu=0, loss=0.062, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   9%|▉         | 16456/179200 [26:12<4:34:36,  9.88batch/s, batch_nb=16455, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  10%|▉         | 17600/179200 [28:00<3:29:10, 12.88batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17610/179200 [28:01<2:34:37, 17.42batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17621/179200 [28:01<1:56:03, 23.20batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17632/179200 [28:01<1:29:03, 30.24batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17643/179200 [28:01<1:10:12, 38.35batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17654/179200 [28:01<57:00, 47.22batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]  \n",
      "Epoch 4:  10%|▉         | 17665/179200 [28:01<47:46, 56.36batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17676/179200 [28:01<41:18, 65.17batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17687/179200 [28:01<36:45, 73.24batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17698/179200 [28:01<33:34, 80.17batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17709/179200 [28:01<31:22, 85.78batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17720/179200 [28:02<29:50, 90.21batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17731/179200 [28:02<28:42, 93.75batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17742/179200 [28:02<27:57, 96.24batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17753/179200 [28:02<27:25, 98.09batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17764/179200 [28:02<27:03, 99.41batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17775/179200 [28:02<26:47, 100.41batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17786/179200 [28:02<26:29, 101.57batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17797/179200 [28:02<26:18, 102.26batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17808/179200 [28:02<26:12, 102.61batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17819/179200 [28:03<26:25, 101.80batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17830/179200 [28:03<26:32, 101.35batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17841/179200 [28:03<26:36, 101.06batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17852/179200 [28:03<26:40, 100.81batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17863/179200 [28:03<26:46, 100.45batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17874/179200 [28:03<26:47, 100.36batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17885/179200 [28:03<26:44, 100.52batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17896/179200 [28:03<26:45, 100.46batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17907/179200 [28:03<26:40, 100.81batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|▉         | 17918/179200 [28:04<26:23, 101.85batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|█         | 17920/179200 [28:05<26:23, 101.85batch/s, batch_nb=17599, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  10%|█         | 18619/179200 [29:12<3:19:56, 13.39batch/s, batch_nb=18298, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  13%|█▎        | 22859/179200 [36:01<3:20:11, 13.02batch/s, batch_nb=22538, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  14%|█▍        | 24935/179200 [39:30<4:40:38,  9.16batch/s, batch_nb=24614, gpu=0, loss=0.060, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  15%|█▌        | 27261/179200 [43:19<2:54:08, 14.54batch/s, batch_nb=26940, gpu=0, loss=0.059, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  17%|█▋        | 29999/179200 [47:47<3:12:54, 12.89batch/s, batch_nb=29678, gpu=0, loss=0.060, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  18%|█▊        | 32161/179200 [51:18<2:42:43, 15.06batch/s, batch_nb=31840, gpu=0, loss=0.060, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  19%|█▉        | 34320/179200 [54:50<2:46:22, 14.51batch/s, batch_nb=33999, gpu=0, loss=0.060, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  20%|█▉        | 35520/179200 [56:48<3:29:23, 11.44batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35524/179200 [56:49<2:41:41, 14.81batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35534/179200 [56:49<2:00:22, 19.89batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35545/179200 [56:49<1:31:14, 26.24batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35556/179200 [56:49<1:10:55, 33.75batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35567/179200 [56:49<56:44, 42.18batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]  \n",
      "Epoch 4:  20%|█▉        | 35578/179200 [56:49<46:45, 51.19batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35589/179200 [56:49<39:40, 60.33batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35600/179200 [56:49<34:43, 68.92batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35611/179200 [56:49<31:22, 76.28batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35622/179200 [56:49<28:59, 82.53batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35633/179200 [56:50<27:14, 87.84batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35644/179200 [56:50<25:58, 92.09batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35655/179200 [56:50<25:05, 95.32batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35666/179200 [56:50<24:43, 96.74batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35677/179200 [56:50<24:26, 97.85batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35688/179200 [56:50<24:06, 99.24batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35699/179200 [56:50<23:52, 100.17batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35710/179200 [56:50<23:58, 99.74batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55] \n",
      "Epoch 4:  20%|█▉        | 35721/179200 [56:50<23:53, 100.06batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35732/179200 [56:51<23:39, 101.05batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35743/179200 [56:51<23:50, 100.32batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35754/179200 [56:51<23:58, 99.75batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55] \n",
      "Epoch 4:  20%|█▉        | 35765/179200 [56:51<23:51, 100.18batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35776/179200 [56:51<23:57, 99.74batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55] \n",
      "Epoch 4:  20%|█▉        | 35787/179200 [56:51<23:48, 100.37batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35798/179200 [56:51<24:16, 98.45batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55] \n",
      "Epoch 4:  20%|█▉        | 35809/179200 [56:51<24:04, 99.28batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35819/179200 [56:51<24:09, 98.89batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|█▉        | 35830/179200 [56:52<24:02, 99.39batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  20%|██        | 35840/179200 [56:53<24:02, 99.39batch/s, batch_nb=35199, gpu=0, loss=0.061, v_nb=55]\n",
      "Epoch 4:  21%|██        | 36955/179200 [58:45<4:25:55,  8.92batch/s, batch_nb=36314, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  22%|██▏       | 40111/179200 [1:03:58<2:55:37, 13.20batch/s, batch_nb=39470, gpu=0, loss=0.060, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  24%|██▎       | 42279/179200 [1:07:38<3:09:50, 12.02batch/s, batch_nb=41638, gpu=0, loss=0.061, v_nb=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  25%|██▍       | 44081/179200 [1:10:40<2:43:49, 13.75batch/s, batch_nb=43440, gpu=0, loss=0.060, v_nb=55]"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, fast_dev_run=False, max_nb_epochs=10, accumulate_grad_batches=4,\n",
    "                     train_percent_check=1, val_check_interval=0.1, use_amp=True,\n",
    "                     default_save_path='../data')    \n",
    "trainer.fit(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 18201/18201 [09:36<00:00, 31.57batch/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.test(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find memory leak(loading train dataset on every run)\n",
    "# torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "# label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1912.01857.pdf\n",
    "# https://github.com/feidfoe/AdjustBnd4Imbalance/blob/master/cifar.py\n",
    "gamma = 0.1 # hparams for re_scaling https://arxiv.org/pdf/1912.01857.pdf\n",
    "if args.evaluate:\n",
    "    print('\\nEvaluation only')\n",
    "    test_loss, test_acc = test(testloader, model, criterion, \n",
    "                               start_epoch, use_cuda)\n",
    "    print('[w/o RS] Test Loss: %.8f, Test Acc: %.2f%%' % (test_loss, test_acc))\n",
    "\n",
    "    current_state = model.state_dict()\n",
    "    W = current_state['module.fc.weight']\n",
    "\n",
    "    imb_factor = 1. / args.imbalance\n",
    "    img_max = 50000/num_classes\n",
    "    num_sample = [img_max * (imb_factor**(i/(num_classes - 1))) \\\n",
    "                     for i in range(num_classes)]\n",
    "\n",
    "    ns = [ float(n) / max(num_sample) for n in num_sample ]\n",
    "    ns = [ n**gamma for n in ns ]\n",
    "    ns = torch.FloatTensor(ns).unsqueeze(-1).cuda()\n",
    "    new_W = W / ns\n",
    "\n",
    "    current_state['module.fc.weight'] = new_W\n",
    "    model.load_state_dict(current_state)\n",
    "\n",
    "    test_loss, test_acc = test(testloader, model, criterion, \n",
    "                               start_epoch, use_cuda)\n",
    "    print('[w/  RS] Test Loss: %.8f, Test Acc: %.2f%%' % (test_loss, test_acc))\n",
    "\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlnd",
   "language": "python",
   "name": "mlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
